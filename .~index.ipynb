{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits.\n",
    "* Apply L1 and L2 regularization.\n",
    "* Aplly dropout regularization.\n",
    "* Observe and comment on the effect of using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "#Your code here\n",
    "# X_train = \n",
    "# X_test = \n",
    "# y_train = \n",
    "# y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9588 - acc: 0.1568 - val_loss: 1.9459 - val_acc: 0.1600\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9320 - acc: 0.1705 - val_loss: 1.9260 - val_acc: 0.1720\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9122 - acc: 0.1867 - val_loss: 1.9088 - val_acc: 0.1690\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8930 - acc: 0.2017 - val_loss: 1.8915 - val_acc: 0.1980\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8739 - acc: 0.2112 - val_loss: 1.8730 - val_acc: 0.2020\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8538 - acc: 0.2175 - val_loss: 1.8525 - val_acc: 0.2090\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8322 - acc: 0.2309 - val_loss: 1.8304 - val_acc: 0.2140\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8086 - acc: 0.2537 - val_loss: 1.8058 - val_acc: 0.2320\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7828 - acc: 0.2732 - val_loss: 1.7789 - val_acc: 0.2640\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7541 - acc: 0.2947 - val_loss: 1.7489 - val_acc: 0.2940\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7221 - acc: 0.3209 - val_loss: 1.7152 - val_acc: 0.3180\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6863 - acc: 0.3511 - val_loss: 1.6777 - val_acc: 0.3400\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6469 - acc: 0.3839 - val_loss: 1.6366 - val_acc: 0.3740\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6040 - acc: 0.4067 - val_loss: 1.5914 - val_acc: 0.4170\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5587 - acc: 0.4384 - val_loss: 1.5443 - val_acc: 0.4300\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5114 - acc: 0.4665 - val_loss: 1.4956 - val_acc: 0.4740\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4627 - acc: 0.5015 - val_loss: 1.4455 - val_acc: 0.5120\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4140 - acc: 0.5304 - val_loss: 1.3962 - val_acc: 0.5260\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3654 - acc: 0.5553 - val_loss: 1.3476 - val_acc: 0.5480\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3175 - acc: 0.5775 - val_loss: 1.3014 - val_acc: 0.5750\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2710 - acc: 0.5965 - val_loss: 1.2536 - val_acc: 0.5950\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2262 - acc: 0.6128 - val_loss: 1.2095 - val_acc: 0.6200\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1832 - acc: 0.6307 - val_loss: 1.1666 - val_acc: 0.6350\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1425 - acc: 0.6436 - val_loss: 1.1281 - val_acc: 0.6480\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1040 - acc: 0.6553 - val_loss: 1.0895 - val_acc: 0.6550\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0674 - acc: 0.6652 - val_loss: 1.0538 - val_acc: 0.6720\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0333 - acc: 0.6744 - val_loss: 1.0208 - val_acc: 0.6730\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0017 - acc: 0.6832 - val_loss: 0.9917 - val_acc: 0.6870\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9719 - acc: 0.6907 - val_loss: 0.9643 - val_acc: 0.7000\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9441 - acc: 0.6995 - val_loss: 0.9383 - val_acc: 0.7050\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9183 - acc: 0.7087 - val_loss: 0.9144 - val_acc: 0.7100\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8944 - acc: 0.7149 - val_loss: 0.8926 - val_acc: 0.7090\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8722 - acc: 0.7171 - val_loss: 0.8687 - val_acc: 0.7160\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8512 - acc: 0.7257 - val_loss: 0.8503 - val_acc: 0.7170\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8316 - acc: 0.7284 - val_loss: 0.8331 - val_acc: 0.7180\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8142 - acc: 0.7329 - val_loss: 0.8184 - val_acc: 0.7250\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7971 - acc: 0.7393 - val_loss: 0.8032 - val_acc: 0.7240\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7814 - acc: 0.7433 - val_loss: 0.7872 - val_acc: 0.7290\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7665 - acc: 0.7464 - val_loss: 0.7760 - val_acc: 0.7350\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7528 - acc: 0.7508 - val_loss: 0.7632 - val_acc: 0.7370\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7398 - acc: 0.7533 - val_loss: 0.7537 - val_acc: 0.7390\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7275 - acc: 0.7568 - val_loss: 0.7420 - val_acc: 0.7440\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7155 - acc: 0.7601 - val_loss: 0.7352 - val_acc: 0.7410\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7046 - acc: 0.7647 - val_loss: 0.7281 - val_acc: 0.7450\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6945 - acc: 0.7659 - val_loss: 0.7155 - val_acc: 0.7480\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6842 - acc: 0.7688 - val_loss: 0.7135 - val_acc: 0.7490\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6749 - acc: 0.7707 - val_loss: 0.7052 - val_acc: 0.7560\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6656 - acc: 0.7747 - val_loss: 0.6971 - val_acc: 0.7550\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6572 - acc: 0.7737 - val_loss: 0.6937 - val_acc: 0.7560\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6485 - acc: 0.7799 - val_loss: 0.6840 - val_acc: 0.7550\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6404 - acc: 0.7825 - val_loss: 0.6726 - val_acc: 0.7550\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6323 - acc: 0.7851 - val_loss: 0.6764 - val_acc: 0.7570\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6255 - acc: 0.7887 - val_loss: 0.6689 - val_acc: 0.7620\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6181 - acc: 0.7913 - val_loss: 0.6611 - val_acc: 0.7650\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6109 - acc: 0.7929 - val_loss: 0.6548 - val_acc: 0.7590\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6042 - acc: 0.7951 - val_loss: 0.6485 - val_acc: 0.7580\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5975 - acc: 0.7981 - val_loss: 0.6455 - val_acc: 0.7550\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5911 - acc: 0.7993 - val_loss: 0.6443 - val_acc: 0.7600\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5845 - acc: 0.8008 - val_loss: 0.6394 - val_acc: 0.7680\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5785 - acc: 0.8051 - val_loss: 0.6362 - val_acc: 0.7700\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5729 - acc: 0.8065 - val_loss: 0.6302 - val_acc: 0.7660\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5674 - acc: 0.8081 - val_loss: 0.6269 - val_acc: 0.7650\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5612 - acc: 0.8099 - val_loss: 0.6256 - val_acc: 0.7660\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5564 - acc: 0.8113 - val_loss: 0.6199 - val_acc: 0.7660\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5506 - acc: 0.8145 - val_loss: 0.6208 - val_acc: 0.7710\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5457 - acc: 0.8161 - val_loss: 0.6167 - val_acc: 0.7720\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5405 - acc: 0.8188 - val_loss: 0.6115 - val_acc: 0.7700\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5349 - acc: 0.8208 - val_loss: 0.6072 - val_acc: 0.7730\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5304 - acc: 0.8223 - val_loss: 0.6104 - val_acc: 0.7760\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5260 - acc: 0.8240 - val_loss: 0.6024 - val_acc: 0.7710\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5208 - acc: 0.8269 - val_loss: 0.5993 - val_acc: 0.7720\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5162 - acc: 0.8284 - val_loss: 0.6054 - val_acc: 0.7770\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5117 - acc: 0.8307 - val_loss: 0.5944 - val_acc: 0.7790\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5073 - acc: 0.8325 - val_loss: 0.5956 - val_acc: 0.7810\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5025 - acc: 0.8352 - val_loss: 0.5961 - val_acc: 0.7810\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4984 - acc: 0.8340 - val_loss: 0.5965 - val_acc: 0.7780\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4941 - acc: 0.8368 - val_loss: 0.5885 - val_acc: 0.7820\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4899 - acc: 0.8395 - val_loss: 0.5904 - val_acc: 0.7840\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4860 - acc: 0.8388 - val_loss: 0.5878 - val_acc: 0.7790\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4820 - acc: 0.8412 - val_loss: 0.5833 - val_acc: 0.7860\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4779 - acc: 0.8425 - val_loss: 0.5872 - val_acc: 0.7770\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4740 - acc: 0.8448 - val_loss: 0.5809 - val_acc: 0.7840\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4700 - acc: 0.8459 - val_loss: 0.5854 - val_acc: 0.7800\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4663 - acc: 0.8475 - val_loss: 0.5766 - val_acc: 0.7850\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4621 - acc: 0.8501 - val_loss: 0.5755 - val_acc: 0.7840\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4587 - acc: 0.8503 - val_loss: 0.5753 - val_acc: 0.7840\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8515 - val_loss: 0.5738 - val_acc: 0.7860\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4513 - acc: 0.8535 - val_loss: 0.5716 - val_acc: 0.7880\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4478 - acc: 0.8560 - val_loss: 0.5728 - val_acc: 0.7850\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4441 - acc: 0.8569 - val_loss: 0.5719 - val_acc: 0.7880\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4407 - acc: 0.8596 - val_loss: 0.5728 - val_acc: 0.7830\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4376 - acc: 0.8599 - val_loss: 0.5680 - val_acc: 0.7880\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4341 - acc: 0.8609 - val_loss: 0.5689 - val_acc: 0.7880\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4304 - acc: 0.8604 - val_loss: 0.5658 - val_acc: 0.7870\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4277 - acc: 0.8632 - val_loss: 0.5679 - val_acc: 0.7890\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4243 - acc: 0.8647 - val_loss: 0.5628 - val_acc: 0.7910\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4211 - acc: 0.8651 - val_loss: 0.5643 - val_acc: 0.7910\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4178 - acc: 0.8692 - val_loss: 0.5654 - val_acc: 0.7890\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4146 - acc: 0.8681 - val_loss: 0.5655 - val_acc: 0.7940\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4117 - acc: 0.8703 - val_loss: 0.5607 - val_acc: 0.7910\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4081 - acc: 0.8691 - val_loss: 0.5647 - val_acc: 0.7880\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4058 - acc: 0.8720 - val_loss: 0.5595 - val_acc: 0.7970\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4028 - acc: 0.8721 - val_loss: 0.5618 - val_acc: 0.7930\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3994 - acc: 0.8732 - val_loss: 0.5641 - val_acc: 0.7970\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3968 - acc: 0.8743 - val_loss: 0.5561 - val_acc: 0.7920\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3942 - acc: 0.8752 - val_loss: 0.5570 - val_acc: 0.7960\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3907 - acc: 0.8771 - val_loss: 0.5570 - val_acc: 0.7940\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3880 - acc: 0.8783 - val_loss: 0.5611 - val_acc: 0.7980\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3853 - acc: 0.8791 - val_loss: 0.5559 - val_acc: 0.7930\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3825 - acc: 0.8799 - val_loss: 0.5591 - val_acc: 0.7980\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3799 - acc: 0.8807 - val_loss: 0.5569 - val_acc: 0.7960\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3770 - acc: 0.8819 - val_loss: 0.5563 - val_acc: 0.7930\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3740 - acc: 0.8843 - val_loss: 0.5552 - val_acc: 0.7990\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3719 - acc: 0.8832 - val_loss: 0.5556 - val_acc: 0.7950\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3692 - acc: 0.8857 - val_loss: 0.5531 - val_acc: 0.7920\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3664 - acc: 0.8856 - val_loss: 0.5596 - val_acc: 0.7980\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3638 - acc: 0.8869 - val_loss: 0.5516 - val_acc: 0.7950\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3615 - acc: 0.8895 - val_loss: 0.5535 - val_acc: 0.7890\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3588 - acc: 0.8895 - val_loss: 0.5537 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3562 - acc: 0.8891 - val_loss: 0.5566 - val_acc: 0.7990\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3533312609394391, 0.8902666666666667]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6457406918207804, 0.7613333338101705]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9657 - acc: 0.1509 - val_loss: 1.9503 - val_acc: 0.1470\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9432 - acc: 0.1611 - val_loss: 1.9364 - val_acc: 0.1710\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9296 - acc: 0.1785 - val_loss: 1.9254 - val_acc: 0.1790\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9174 - acc: 0.1916 - val_loss: 1.9148 - val_acc: 0.2000\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9049 - acc: 0.2056 - val_loss: 1.9025 - val_acc: 0.2190\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8906 - acc: 0.2207 - val_loss: 1.8885 - val_acc: 0.2280\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8742 - acc: 0.2352 - val_loss: 1.8713 - val_acc: 0.2470\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8546 - acc: 0.2595 - val_loss: 1.8509 - val_acc: 0.2590\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8317 - acc: 0.2812 - val_loss: 1.8267 - val_acc: 0.2850\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8048 - acc: 0.3121 - val_loss: 1.7979 - val_acc: 0.3170\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7728 - acc: 0.3461 - val_loss: 1.7641 - val_acc: 0.3470\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7360 - acc: 0.3817 - val_loss: 1.7252 - val_acc: 0.3750\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6950 - acc: 0.4089 - val_loss: 1.6818 - val_acc: 0.4110\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6497 - acc: 0.4397 - val_loss: 1.6354 - val_acc: 0.4440\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6016 - acc: 0.4727 - val_loss: 1.5861 - val_acc: 0.4640\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5513 - acc: 0.5025 - val_loss: 1.5353 - val_acc: 0.4870\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5011 - acc: 0.5207 - val_loss: 1.4849 - val_acc: 0.5180\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4503 - acc: 0.5389 - val_loss: 1.4358 - val_acc: 0.5300\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4009 - acc: 0.5547 - val_loss: 1.3879 - val_acc: 0.5450\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3535 - acc: 0.5679 - val_loss: 1.3431 - val_acc: 0.5650\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3081 - acc: 0.5845 - val_loss: 1.2988 - val_acc: 0.5890\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2646 - acc: 0.6003 - val_loss: 1.2573 - val_acc: 0.6020\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2240 - acc: 0.6140 - val_loss: 1.2186 - val_acc: 0.6120\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1846 - acc: 0.6301 - val_loss: 1.1828 - val_acc: 0.6260\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1483 - acc: 0.6417 - val_loss: 1.1477 - val_acc: 0.6460\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1129 - acc: 0.6527 - val_loss: 1.1142 - val_acc: 0.6450\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0802 - acc: 0.6615 - val_loss: 1.0845 - val_acc: 0.6590\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0490 - acc: 0.6708 - val_loss: 1.0571 - val_acc: 0.6650\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0189 - acc: 0.6836 - val_loss: 1.0290 - val_acc: 0.6700\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9916 - acc: 0.6896 - val_loss: 1.0036 - val_acc: 0.6820\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9651 - acc: 0.7013 - val_loss: 0.9814 - val_acc: 0.6790\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9400 - acc: 0.7067 - val_loss: 0.9566 - val_acc: 0.6940\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9167 - acc: 0.7131 - val_loss: 0.9353 - val_acc: 0.6990\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8943 - acc: 0.7188 - val_loss: 0.9154 - val_acc: 0.7060\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8732 - acc: 0.7256 - val_loss: 0.8969 - val_acc: 0.7090\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8532 - acc: 0.7303 - val_loss: 0.8791 - val_acc: 0.7180\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8346 - acc: 0.7353 - val_loss: 0.8627 - val_acc: 0.7160\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8168 - acc: 0.7400 - val_loss: 0.8511 - val_acc: 0.7180\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8002 - acc: 0.7432 - val_loss: 0.8324 - val_acc: 0.7290\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7845 - acc: 0.7472 - val_loss: 0.8190 - val_acc: 0.7300\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7694 - acc: 0.7517 - val_loss: 0.8053 - val_acc: 0.7330\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7546 - acc: 0.7532 - val_loss: 0.7923 - val_acc: 0.7330\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7408 - acc: 0.7585 - val_loss: 0.7832 - val_acc: 0.7370\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7281 - acc: 0.7595 - val_loss: 0.7702 - val_acc: 0.7390\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7161 - acc: 0.7656 - val_loss: 0.7610 - val_acc: 0.7490\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7044 - acc: 0.7672 - val_loss: 0.7508 - val_acc: 0.7420\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6933 - acc: 0.7707 - val_loss: 0.7430 - val_acc: 0.7460\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6824 - acc: 0.7745 - val_loss: 0.7354 - val_acc: 0.7430\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6723 - acc: 0.7781 - val_loss: 0.7267 - val_acc: 0.7480\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6618 - acc: 0.7812 - val_loss: 0.7174 - val_acc: 0.7540\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6530 - acc: 0.7847 - val_loss: 0.7112 - val_acc: 0.7580\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6441 - acc: 0.7863 - val_loss: 0.7036 - val_acc: 0.7560\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6352 - acc: 0.7883 - val_loss: 0.6958 - val_acc: 0.7640\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6271 - acc: 0.7915 - val_loss: 0.6918 - val_acc: 0.7620\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6189 - acc: 0.7953 - val_loss: 0.6834 - val_acc: 0.7650\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6107 - acc: 0.7957 - val_loss: 0.6783 - val_acc: 0.7680\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6041 - acc: 0.7977 - val_loss: 0.6743 - val_acc: 0.7670\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5965 - acc: 0.8009 - val_loss: 0.6688 - val_acc: 0.7660\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5899 - acc: 0.8041 - val_loss: 0.6621 - val_acc: 0.7660\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5828 - acc: 0.8083 - val_loss: 0.6587 - val_acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 65us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5770250624895096, 0.81]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6961977109909058, 0.7433333336512248]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.5921 - acc: 0.1651 - val_loss: 2.5829 - val_acc: 0.1870\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5717 - acc: 0.1805 - val_loss: 2.5666 - val_acc: 0.2130\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5532 - acc: 0.2024 - val_loss: 2.5502 - val_acc: 0.2290\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5338 - acc: 0.2289 - val_loss: 2.5323 - val_acc: 0.2680\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5126 - acc: 0.2564 - val_loss: 2.5119 - val_acc: 0.2800\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.4898 - acc: 0.2828 - val_loss: 2.4895 - val_acc: 0.3010\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.4655 - acc: 0.3061 - val_loss: 2.4649 - val_acc: 0.3200\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.4389 - acc: 0.3225 - val_loss: 2.4379 - val_acc: 0.3400\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.4099 - acc: 0.3508 - val_loss: 2.4076 - val_acc: 0.3450\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.3780 - acc: 0.3800 - val_loss: 2.3753 - val_acc: 0.3610\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.3436 - acc: 0.4027 - val_loss: 2.3391 - val_acc: 0.3860\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.3068 - acc: 0.4291 - val_loss: 2.3004 - val_acc: 0.4130\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.2675 - acc: 0.4532 - val_loss: 2.2593 - val_acc: 0.4470\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2264 - acc: 0.4816 - val_loss: 2.2165 - val_acc: 0.4790\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1840 - acc: 0.5036 - val_loss: 2.1728 - val_acc: 0.4930\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1402 - acc: 0.5244 - val_loss: 2.1282 - val_acc: 0.5180\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0958 - acc: 0.5452 - val_loss: 2.0826 - val_acc: 0.5350\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0508 - acc: 0.5667 - val_loss: 2.0368 - val_acc: 0.5600\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0054 - acc: 0.5868 - val_loss: 1.9916 - val_acc: 0.5760\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9604 - acc: 0.6059 - val_loss: 1.9459 - val_acc: 0.5930\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9158 - acc: 0.6232 - val_loss: 1.9019 - val_acc: 0.5980\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8713 - acc: 0.6389 - val_loss: 1.8581 - val_acc: 0.6250\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8283 - acc: 0.6513 - val_loss: 1.8158 - val_acc: 0.6450\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7869 - acc: 0.6667 - val_loss: 1.7752 - val_acc: 0.6570\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7466 - acc: 0.6773 - val_loss: 1.7374 - val_acc: 0.6720\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7081 - acc: 0.6868 - val_loss: 1.7011 - val_acc: 0.6830\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6713 - acc: 0.6955 - val_loss: 1.6645 - val_acc: 0.6910\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6366 - acc: 0.7032 - val_loss: 1.6306 - val_acc: 0.7000\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6035 - acc: 0.7105 - val_loss: 1.6004 - val_acc: 0.7170\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5724 - acc: 0.7201 - val_loss: 1.5710 - val_acc: 0.7160\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5431 - acc: 0.7249 - val_loss: 1.5442 - val_acc: 0.7180\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5158 - acc: 0.7323 - val_loss: 1.5196 - val_acc: 0.7290\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4905 - acc: 0.7348 - val_loss: 1.4962 - val_acc: 0.7260\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4663 - acc: 0.7417 - val_loss: 1.4750 - val_acc: 0.7310\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4440 - acc: 0.7456 - val_loss: 1.4524 - val_acc: 0.7360\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4227 - acc: 0.7479 - val_loss: 1.4328 - val_acc: 0.7380\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4026 - acc: 0.7523 - val_loss: 1.4156 - val_acc: 0.7410\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3840 - acc: 0.7549 - val_loss: 1.3990 - val_acc: 0.7350\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3659 - acc: 0.7581 - val_loss: 1.3822 - val_acc: 0.7450\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3495 - acc: 0.7628 - val_loss: 1.3686 - val_acc: 0.7450\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3337 - acc: 0.7673 - val_loss: 1.3557 - val_acc: 0.7500\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3191 - acc: 0.7673 - val_loss: 1.3424 - val_acc: 0.7510\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3042 - acc: 0.7707 - val_loss: 1.3307 - val_acc: 0.7450\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2909 - acc: 0.7741 - val_loss: 1.3195 - val_acc: 0.7520\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2784 - acc: 0.7772 - val_loss: 1.3081 - val_acc: 0.7590\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2662 - acc: 0.7791 - val_loss: 1.2972 - val_acc: 0.7540\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2543 - acc: 0.7836 - val_loss: 1.2856 - val_acc: 0.7510\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2431 - acc: 0.7848 - val_loss: 1.2792 - val_acc: 0.7610\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2326 - acc: 0.7897 - val_loss: 1.2731 - val_acc: 0.7530\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2219 - acc: 0.7901 - val_loss: 1.2612 - val_acc: 0.7640\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2116 - acc: 0.7923 - val_loss: 1.2512 - val_acc: 0.7570\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2022 - acc: 0.7925 - val_loss: 1.2451 - val_acc: 0.7640\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1926 - acc: 0.7953 - val_loss: 1.2386 - val_acc: 0.7730\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1839 - acc: 0.7985 - val_loss: 1.2307 - val_acc: 0.7670\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1749 - acc: 0.7989 - val_loss: 1.2260 - val_acc: 0.7600\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1666 - acc: 0.8008 - val_loss: 1.2217 - val_acc: 0.7600\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1581 - acc: 0.8035 - val_loss: 1.2107 - val_acc: 0.7650\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1504 - acc: 0.8065 - val_loss: 1.2057 - val_acc: 0.7700\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1428 - acc: 0.8056 - val_loss: 1.1987 - val_acc: 0.7690\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1350 - acc: 0.8095 - val_loss: 1.1920 - val_acc: 0.7710\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1272 - acc: 0.8117 - val_loss: 1.1881 - val_acc: 0.7630\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1200 - acc: 0.8132 - val_loss: 1.1827 - val_acc: 0.7620\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1128 - acc: 0.8143 - val_loss: 1.1765 - val_acc: 0.7710\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1056 - acc: 0.8172 - val_loss: 1.1755 - val_acc: 0.7650\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0991 - acc: 0.8181 - val_loss: 1.1663 - val_acc: 0.7690\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0924 - acc: 0.8223 - val_loss: 1.1609 - val_acc: 0.7700\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0859 - acc: 0.8227 - val_loss: 1.1555 - val_acc: 0.7800\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0795 - acc: 0.8237 - val_loss: 1.1561 - val_acc: 0.7620\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0732 - acc: 0.8247 - val_loss: 1.1504 - val_acc: 0.7750\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0668 - acc: 0.8276 - val_loss: 1.1472 - val_acc: 0.7730\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0610 - acc: 0.8283 - val_loss: 1.1414 - val_acc: 0.7660\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0550 - acc: 0.8304 - val_loss: 1.1363 - val_acc: 0.7770\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0493 - acc: 0.8305 - val_loss: 1.1362 - val_acc: 0.7710\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0434 - acc: 0.8333 - val_loss: 1.1275 - val_acc: 0.7760\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0374 - acc: 0.8371 - val_loss: 1.1226 - val_acc: 0.7830\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0320 - acc: 0.8352 - val_loss: 1.1203 - val_acc: 0.7660\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0262 - acc: 0.8372 - val_loss: 1.1171 - val_acc: 0.7770\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0207 - acc: 0.8384 - val_loss: 1.1187 - val_acc: 0.7670\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0158 - acc: 0.8409 - val_loss: 1.1095 - val_acc: 0.7720\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0105 - acc: 0.8409 - val_loss: 1.1043 - val_acc: 0.7870\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0051 - acc: 0.8424 - val_loss: 1.1017 - val_acc: 0.7840\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9998 - acc: 0.8455 - val_loss: 1.1003 - val_acc: 0.7720\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9949 - acc: 0.8433 - val_loss: 1.0949 - val_acc: 0.7860\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9895 - acc: 0.8463 - val_loss: 1.0930 - val_acc: 0.7850\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9847 - acc: 0.8460 - val_loss: 1.0888 - val_acc: 0.7780\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9800 - acc: 0.8485 - val_loss: 1.0867 - val_acc: 0.7770\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9749 - acc: 0.8477 - val_loss: 1.0838 - val_acc: 0.7870\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9704 - acc: 0.8495 - val_loss: 1.0808 - val_acc: 0.7770\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9654 - acc: 0.8507 - val_loss: 1.0786 - val_acc: 0.7810\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9609 - acc: 0.8527 - val_loss: 1.0753 - val_acc: 0.7800\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9561 - acc: 0.8540 - val_loss: 1.0739 - val_acc: 0.7790\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9515 - acc: 0.8547 - val_loss: 1.0686 - val_acc: 0.7810\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9469 - acc: 0.8567 - val_loss: 1.0647 - val_acc: 0.7760\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9432 - acc: 0.8551 - val_loss: 1.0631 - val_acc: 0.7870\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9383 - acc: 0.8569 - val_loss: 1.0604 - val_acc: 0.7820\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9337 - acc: 0.8580 - val_loss: 1.0584 - val_acc: 0.7860\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9297 - acc: 0.8603 - val_loss: 1.0549 - val_acc: 0.7800\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9253 - acc: 0.8604 - val_loss: 1.0544 - val_acc: 0.7870\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9210 - acc: 0.8619 - val_loss: 1.0496 - val_acc: 0.7910\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9175 - acc: 0.8632 - val_loss: 1.0471 - val_acc: 0.7850\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9131 - acc: 0.8628 - val_loss: 1.0444 - val_acc: 0.7880\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9092 - acc: 0.8636 - val_loss: 1.0419 - val_acc: 0.7820\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9051 - acc: 0.8651 - val_loss: 1.0422 - val_acc: 0.7840\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9011 - acc: 0.8668 - val_loss: 1.0375 - val_acc: 0.7860\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8970 - acc: 0.8667 - val_loss: 1.0363 - val_acc: 0.7840\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8930 - acc: 0.8701 - val_loss: 1.0352 - val_acc: 0.7910\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8893 - acc: 0.8701 - val_loss: 1.0330 - val_acc: 0.7960\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8853 - acc: 0.8719 - val_loss: 1.0294 - val_acc: 0.7890\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8821 - acc: 0.8713 - val_loss: 1.0272 - val_acc: 0.7830\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8780 - acc: 0.8727 - val_loss: 1.0233 - val_acc: 0.7890\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8741 - acc: 0.8727 - val_loss: 1.0231 - val_acc: 0.7950\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8704 - acc: 0.8733 - val_loss: 1.0180 - val_acc: 0.7930\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8666 - acc: 0.8757 - val_loss: 1.0203 - val_acc: 0.7840\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8632 - acc: 0.8767 - val_loss: 1.0181 - val_acc: 0.7900\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8597 - acc: 0.8753 - val_loss: 1.0149 - val_acc: 0.7980\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8559 - acc: 0.8779 - val_loss: 1.0134 - val_acc: 0.7900\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8524 - acc: 0.8784 - val_loss: 1.0086 - val_acc: 0.7980\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8490 - acc: 0.8789 - val_loss: 1.0086 - val_acc: 0.7900\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8455 - acc: 0.8793 - val_loss: 1.0048 - val_acc: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8422 - acc: 0.8815 - val_loss: 1.0027 - val_acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 15.9638 - acc: 0.1525 - val_loss: 15.5482 - val_acc: 0.1850\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 15.1924 - acc: 0.2135 - val_loss: 14.7964 - val_acc: 0.2520\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 14.4507 - acc: 0.2661 - val_loss: 14.0690 - val_acc: 0.2860\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 13.7306 - acc: 0.2925 - val_loss: 13.3598 - val_acc: 0.3070\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 13.0304 - acc: 0.3187 - val_loss: 12.6695 - val_acc: 0.3300\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 12.3498 - acc: 0.3481 - val_loss: 11.9992 - val_acc: 0.3560\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 11.6896 - acc: 0.3835 - val_loss: 11.3497 - val_acc: 0.3890\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 11.0502 - acc: 0.4177 - val_loss: 10.7211 - val_acc: 0.4420\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 10.4317 - acc: 0.4647 - val_loss: 10.1130 - val_acc: 0.4690\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 9.8341 - acc: 0.4893 - val_loss: 9.5272 - val_acc: 0.5020\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 9.2581 - acc: 0.5272 - val_loss: 8.9616 - val_acc: 0.5430\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 8.7033 - acc: 0.5567 - val_loss: 8.4186 - val_acc: 0.5730\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 8.1709 - acc: 0.5749 - val_loss: 7.8986 - val_acc: 0.5930\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 7.6611 - acc: 0.5984 - val_loss: 7.4000 - val_acc: 0.6230\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 7.1732 - acc: 0.6144 - val_loss: 6.9244 - val_acc: 0.6300\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.7082 - acc: 0.6269 - val_loss: 6.4722 - val_acc: 0.6320\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 6.2658 - acc: 0.6340 - val_loss: 6.0411 - val_acc: 0.6490\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 5.8457 - acc: 0.6500 - val_loss: 5.6326 - val_acc: 0.6580\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 5.4481 - acc: 0.6545 - val_loss: 5.2496 - val_acc: 0.6700\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 5.0735 - acc: 0.6644 - val_loss: 4.8835 - val_acc: 0.6770\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 4.7210 - acc: 0.6712 - val_loss: 4.5442 - val_acc: 0.6710\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 4.3916 - acc: 0.6740 - val_loss: 4.2237 - val_acc: 0.6760\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 4.0845 - acc: 0.6812 - val_loss: 3.9287 - val_acc: 0.6760\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.8006 - acc: 0.6836 - val_loss: 3.6581 - val_acc: 0.6830\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.5396 - acc: 0.6863 - val_loss: 3.4084 - val_acc: 0.6790\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.3012 - acc: 0.6880 - val_loss: 3.1798 - val_acc: 0.6860\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.0849 - acc: 0.6880 - val_loss: 2.9742 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.8903 - acc: 0.6876 - val_loss: 2.7888 - val_acc: 0.6920\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.7174 - acc: 0.6883 - val_loss: 2.6264 - val_acc: 0.6910\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.5648 - acc: 0.6875 - val_loss: 2.4838 - val_acc: 0.6940\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4322 - acc: 0.6864 - val_loss: 2.3591 - val_acc: 0.6940\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.3194 - acc: 0.6876 - val_loss: 2.2566 - val_acc: 0.7030\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.2261 - acc: 0.6851 - val_loss: 2.1710 - val_acc: 0.6990\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.1506 - acc: 0.6855 - val_loss: 2.1028 - val_acc: 0.7030\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0923 - acc: 0.6840 - val_loss: 2.0505 - val_acc: 0.7020\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0480 - acc: 0.6840 - val_loss: 2.0133 - val_acc: 0.7010\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0151 - acc: 0.6828 - val_loss: 1.9843 - val_acc: 0.7020\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9885 - acc: 0.6825 - val_loss: 1.9567 - val_acc: 0.7040\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9658 - acc: 0.6813 - val_loss: 1.9328 - val_acc: 0.7010\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9455 - acc: 0.6817 - val_loss: 1.9150 - val_acc: 0.7010\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9265 - acc: 0.6799 - val_loss: 1.8945 - val_acc: 0.7050\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9091 - acc: 0.6815 - val_loss: 1.8770 - val_acc: 0.7100\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8930 - acc: 0.6820 - val_loss: 1.8626 - val_acc: 0.6980\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8773 - acc: 0.6805 - val_loss: 1.8460 - val_acc: 0.7080\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8632 - acc: 0.6809 - val_loss: 1.8344 - val_acc: 0.7100\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8497 - acc: 0.6817 - val_loss: 1.8156 - val_acc: 0.7090\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8356 - acc: 0.6845 - val_loss: 1.8096 - val_acc: 0.7010\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8237 - acc: 0.6825 - val_loss: 1.7891 - val_acc: 0.7110\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8106 - acc: 0.6820 - val_loss: 1.7782 - val_acc: 0.7070\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7986 - acc: 0.6833 - val_loss: 1.7700 - val_acc: 0.7060\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7877 - acc: 0.6813 - val_loss: 1.7547 - val_acc: 0.7090\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7758 - acc: 0.6860 - val_loss: 1.7428 - val_acc: 0.7120\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7655 - acc: 0.6836 - val_loss: 1.7362 - val_acc: 0.7120\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7545 - acc: 0.6861 - val_loss: 1.7206 - val_acc: 0.7090\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7445 - acc: 0.6851 - val_loss: 1.7078 - val_acc: 0.7110\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7340 - acc: 0.6840 - val_loss: 1.6980 - val_acc: 0.7130\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7243 - acc: 0.6872 - val_loss: 1.6894 - val_acc: 0.7090\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7147 - acc: 0.6864 - val_loss: 1.6774 - val_acc: 0.7120\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7047 - acc: 0.6877 - val_loss: 1.6665 - val_acc: 0.7150\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6960 - acc: 0.6895 - val_loss: 1.6596 - val_acc: 0.7130\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6865 - acc: 0.6877 - val_loss: 1.6531 - val_acc: 0.7110\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6782 - acc: 0.6901 - val_loss: 1.6428 - val_acc: 0.7120\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6693 - acc: 0.6889 - val_loss: 1.6330 - val_acc: 0.7150\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6607 - acc: 0.6887 - val_loss: 1.6259 - val_acc: 0.7160\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6524 - acc: 0.6905 - val_loss: 1.6149 - val_acc: 0.7170\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6438 - acc: 0.6915 - val_loss: 1.6048 - val_acc: 0.7100\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6360 - acc: 0.6904 - val_loss: 1.5973 - val_acc: 0.7110\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6276 - acc: 0.6921 - val_loss: 1.5897 - val_acc: 0.7240\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6199 - acc: 0.6925 - val_loss: 1.5833 - val_acc: 0.7160\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6124 - acc: 0.6919 - val_loss: 1.5725 - val_acc: 0.7140\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6044 - acc: 0.6929 - val_loss: 1.5672 - val_acc: 0.7160\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5974 - acc: 0.6937 - val_loss: 1.5609 - val_acc: 0.7130\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5896 - acc: 0.6953 - val_loss: 1.5498 - val_acc: 0.7160\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5824 - acc: 0.6936 - val_loss: 1.5432 - val_acc: 0.7150\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5748 - acc: 0.6955 - val_loss: 1.5351 - val_acc: 0.7200\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5674 - acc: 0.6945 - val_loss: 1.5270 - val_acc: 0.7210\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5606 - acc: 0.6959 - val_loss: 1.5253 - val_acc: 0.7210\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5535 - acc: 0.6965 - val_loss: 1.5157 - val_acc: 0.7210\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5464 - acc: 0.6975 - val_loss: 1.5059 - val_acc: 0.7240\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5400 - acc: 0.6983 - val_loss: 1.5003 - val_acc: 0.7190\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5333 - acc: 0.6977 - val_loss: 1.4944 - val_acc: 0.7150\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5263 - acc: 0.6983 - val_loss: 1.4901 - val_acc: 0.7170\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5200 - acc: 0.6981 - val_loss: 1.4789 - val_acc: 0.7160\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5130 - acc: 0.6973 - val_loss: 1.4736 - val_acc: 0.7180\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5073 - acc: 0.6980 - val_loss: 1.4652 - val_acc: 0.7290\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5003 - acc: 0.6985 - val_loss: 1.4589 - val_acc: 0.7230\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4938 - acc: 0.6995 - val_loss: 1.4551 - val_acc: 0.7240\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4873 - acc: 0.7003 - val_loss: 1.4470 - val_acc: 0.7210\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4817 - acc: 0.6988 - val_loss: 1.4431 - val_acc: 0.7170\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4754 - acc: 0.7007 - val_loss: 1.4345 - val_acc: 0.7190\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4690 - acc: 0.7003 - val_loss: 1.4281 - val_acc: 0.7250\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4629 - acc: 0.7017 - val_loss: 1.4244 - val_acc: 0.7270\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4574 - acc: 0.7009 - val_loss: 1.4185 - val_acc: 0.7220\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4515 - acc: 0.7012 - val_loss: 1.4159 - val_acc: 0.7230\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4455 - acc: 0.7012 - val_loss: 1.4096 - val_acc: 0.7220\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4401 - acc: 0.7027 - val_loss: 1.3994 - val_acc: 0.7250\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4341 - acc: 0.7032 - val_loss: 1.3959 - val_acc: 0.7240\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4279 - acc: 0.7037 - val_loss: 1.3886 - val_acc: 0.7370\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4228 - acc: 0.7035 - val_loss: 1.3834 - val_acc: 0.7310\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4173 - acc: 0.7027 - val_loss: 1.3811 - val_acc: 0.7300\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4117 - acc: 0.7031 - val_loss: 1.3736 - val_acc: 0.7340\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4066 - acc: 0.7019 - val_loss: 1.3653 - val_acc: 0.7290\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4007 - acc: 0.7032 - val_loss: 1.3607 - val_acc: 0.7310\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3954 - acc: 0.7056 - val_loss: 1.3573 - val_acc: 0.7310\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3904 - acc: 0.7045 - val_loss: 1.3537 - val_acc: 0.7270\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3857 - acc: 0.7047 - val_loss: 1.3463 - val_acc: 0.7280\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3804 - acc: 0.7053 - val_loss: 1.3419 - val_acc: 0.7370\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3759 - acc: 0.7052 - val_loss: 1.3389 - val_acc: 0.7290\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3704 - acc: 0.7055 - val_loss: 1.3341 - val_acc: 0.7290\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3654 - acc: 0.7059 - val_loss: 1.3271 - val_acc: 0.7390\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3601 - acc: 0.7061 - val_loss: 1.3231 - val_acc: 0.7330\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3552 - acc: 0.7055 - val_loss: 1.3190 - val_acc: 0.7270\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3508 - acc: 0.7072 - val_loss: 1.3114 - val_acc: 0.7350\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3462 - acc: 0.7071 - val_loss: 1.3073 - val_acc: 0.7330\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3412 - acc: 0.7084 - val_loss: 1.3080 - val_acc: 0.7330\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3370 - acc: 0.7083 - val_loss: 1.2966 - val_acc: 0.7330\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3316 - acc: 0.7087 - val_loss: 1.2959 - val_acc: 0.7330\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3274 - acc: 0.7083 - val_loss: 1.2918 - val_acc: 0.7330\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3234 - acc: 0.7084 - val_loss: 1.2859 - val_acc: 0.7290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3187 - acc: 0.7087 - val_loss: 1.2816 - val_acc: 0.7350\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 16.0260 - acc: 0.1976 - val_loss: 15.6143 - val_acc: 0.2050\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 15.2561 - acc: 0.2185 - val_loss: 14.8637 - val_acc: 0.2280\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 14.5146 - acc: 0.2257 - val_loss: 14.1351 - val_acc: 0.2420\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 13.7951 - acc: 0.2411 - val_loss: 13.4273 - val_acc: 0.2570\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 13.0950 - acc: 0.2647 - val_loss: 12.7383 - val_acc: 0.2790\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 12.4139 - acc: 0.2860 - val_loss: 12.0683 - val_acc: 0.3130\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.7517 - acc: 0.3189 - val_loss: 11.4161 - val_acc: 0.3380\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.1084 - acc: 0.3500 - val_loss: 10.7841 - val_acc: 0.3680\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 10.4855 - acc: 0.3837 - val_loss: 10.1725 - val_acc: 0.3820\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 9.8832 - acc: 0.4156 - val_loss: 9.5809 - val_acc: 0.4230\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 9.3015 - acc: 0.4497 - val_loss: 9.0107 - val_acc: 0.4450\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 8.7401 - acc: 0.4851 - val_loss: 8.4607 - val_acc: 0.4960\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 8.2009 - acc: 0.5260 - val_loss: 7.9325 - val_acc: 0.5190\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 7.6839 - acc: 0.5545 - val_loss: 7.4281 - val_acc: 0.5340\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 7.1907 - acc: 0.5816 - val_loss: 6.9466 - val_acc: 0.5650\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 6.7205 - acc: 0.6025 - val_loss: 6.4888 - val_acc: 0.5880\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 6.2748 - acc: 0.6195 - val_loss: 6.0569 - val_acc: 0.6030\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.8537 - acc: 0.6293 - val_loss: 5.6460 - val_acc: 0.6380\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 5.4562 - acc: 0.6441 - val_loss: 5.2615 - val_acc: 0.6340\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 5.0832 - acc: 0.6560 - val_loss: 4.8998 - val_acc: 0.6460\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 4.7329 - acc: 0.6619 - val_loss: 4.5611 - val_acc: 0.6600\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 4.4059 - acc: 0.6705 - val_loss: 4.2454 - val_acc: 0.6630\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 4.1014 - acc: 0.6749 - val_loss: 3.9524 - val_acc: 0.6810\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 3.8199 - acc: 0.6803 - val_loss: 3.6802 - val_acc: 0.6770\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.5605 - acc: 0.6817 - val_loss: 3.4331 - val_acc: 0.6810\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 3.3239 - acc: 0.6845 - val_loss: 3.2070 - val_acc: 0.6950\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.1088 - acc: 0.6863 - val_loss: 3.0028 - val_acc: 0.6840\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.9154 - acc: 0.6901 - val_loss: 2.8228 - val_acc: 0.6770\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.7434 - acc: 0.6891 - val_loss: 2.6546 - val_acc: 0.6990\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5923 - acc: 0.6900 - val_loss: 2.5145 - val_acc: 0.6860\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4614 - acc: 0.6892 - val_loss: 2.3918 - val_acc: 0.6930\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.3503 - acc: 0.6880 - val_loss: 2.2910 - val_acc: 0.7070\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2588 - acc: 0.6884 - val_loss: 2.2069 - val_acc: 0.7020\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1846 - acc: 0.6892 - val_loss: 2.1399 - val_acc: 0.7040\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1270 - acc: 0.6873 - val_loss: 2.0900 - val_acc: 0.7070\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0836 - acc: 0.6881 - val_loss: 2.0495 - val_acc: 0.7030\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0507 - acc: 0.6855 - val_loss: 2.0193 - val_acc: 0.7010\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.0246 - acc: 0.6852 - val_loss: 1.9953 - val_acc: 0.6980\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0018 - acc: 0.6859 - val_loss: 1.9708 - val_acc: 0.7120\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9820 - acc: 0.6859 - val_loss: 1.9511 - val_acc: 0.7060\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9634 - acc: 0.6868 - val_loss: 1.9315 - val_acc: 0.7060\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9459 - acc: 0.6855 - val_loss: 1.9146 - val_acc: 0.7150\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9299 - acc: 0.6851 - val_loss: 1.8984 - val_acc: 0.7090\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9139 - acc: 0.6837 - val_loss: 1.8864 - val_acc: 0.7070\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8994 - acc: 0.6849 - val_loss: 1.8688 - val_acc: 0.7120\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8857 - acc: 0.6884 - val_loss: 1.8546 - val_acc: 0.7140\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8721 - acc: 0.6860 - val_loss: 1.8412 - val_acc: 0.7200\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8593 - acc: 0.6871 - val_loss: 1.8269 - val_acc: 0.7160\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8468 - acc: 0.6872 - val_loss: 1.8136 - val_acc: 0.7180\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8342 - acc: 0.6863 - val_loss: 1.8022 - val_acc: 0.7150\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8228 - acc: 0.6876 - val_loss: 1.7892 - val_acc: 0.7160\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8108 - acc: 0.6872 - val_loss: 1.7776 - val_acc: 0.7210\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7996 - acc: 0.6863 - val_loss: 1.7659 - val_acc: 0.7200\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7888 - acc: 0.6889 - val_loss: 1.7547 - val_acc: 0.7220\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7785 - acc: 0.6875 - val_loss: 1.7471 - val_acc: 0.7110\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7675 - acc: 0.6873 - val_loss: 1.7343 - val_acc: 0.7150\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7575 - acc: 0.6893 - val_loss: 1.7219 - val_acc: 0.7170\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7472 - acc: 0.6881 - val_loss: 1.7155 - val_acc: 0.7140\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7372 - acc: 0.6904 - val_loss: 1.7026 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7282 - acc: 0.6896 - val_loss: 1.6984 - val_acc: 0.7110\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7187 - acc: 0.6909 - val_loss: 1.6847 - val_acc: 0.7210\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.7096 - acc: 0.6896 - val_loss: 1.6754 - val_acc: 0.7260\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7005 - acc: 0.6909 - val_loss: 1.6691 - val_acc: 0.7260\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6917 - acc: 0.6909 - val_loss: 1.6568 - val_acc: 0.7250\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6830 - acc: 0.6908 - val_loss: 1.6474 - val_acc: 0.7230\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6743 - acc: 0.6903 - val_loss: 1.6385 - val_acc: 0.7250\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6663 - acc: 0.6919 - val_loss: 1.6318 - val_acc: 0.7230\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6583 - acc: 0.6945 - val_loss: 1.6240 - val_acc: 0.7210\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6492 - acc: 0.6939 - val_loss: 1.6124 - val_acc: 0.7250\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6416 - acc: 0.6947 - val_loss: 1.6071 - val_acc: 0.7290\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6336 - acc: 0.6953 - val_loss: 1.5965 - val_acc: 0.7320\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6258 - acc: 0.6931 - val_loss: 1.5930 - val_acc: 0.7270\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6189 - acc: 0.6965 - val_loss: 1.5807 - val_acc: 0.7270\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6109 - acc: 0.6977 - val_loss: 1.5748 - val_acc: 0.7320\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6033 - acc: 0.6956 - val_loss: 1.5651 - val_acc: 0.7330\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5961 - acc: 0.6963 - val_loss: 1.5591 - val_acc: 0.7360\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5889 - acc: 0.6972 - val_loss: 1.5497 - val_acc: 0.7300\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5815 - acc: 0.6969 - val_loss: 1.5450 - val_acc: 0.7310\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5748 - acc: 0.6996 - val_loss: 1.5371 - val_acc: 0.7290\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5675 - acc: 0.7007 - val_loss: 1.5304 - val_acc: 0.7290\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5609 - acc: 0.7001 - val_loss: 1.5209 - val_acc: 0.7370\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5541 - acc: 0.6999 - val_loss: 1.5155 - val_acc: 0.7350\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5471 - acc: 0.7000 - val_loss: 1.5117 - val_acc: 0.7310\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5405 - acc: 0.7003 - val_loss: 1.5143 - val_acc: 0.7260\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5345 - acc: 0.7023 - val_loss: 1.5002 - val_acc: 0.7280\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5275 - acc: 0.7027 - val_loss: 1.4881 - val_acc: 0.7350\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5209 - acc: 0.7016 - val_loss: 1.4817 - val_acc: 0.7340\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5146 - acc: 0.7024 - val_loss: 1.4779 - val_acc: 0.7330\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5086 - acc: 0.7045 - val_loss: 1.4672 - val_acc: 0.7350\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5021 - acc: 0.7036 - val_loss: 1.4648 - val_acc: 0.7330\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4962 - acc: 0.7041 - val_loss: 1.4569 - val_acc: 0.7310\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4898 - acc: 0.7043 - val_loss: 1.4491 - val_acc: 0.7360\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4838 - acc: 0.7045 - val_loss: 1.4456 - val_acc: 0.7370\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4781 - acc: 0.7036 - val_loss: 1.4383 - val_acc: 0.7370\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4720 - acc: 0.7055 - val_loss: 1.4309 - val_acc: 0.7370\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4656 - acc: 0.7064 - val_loss: 1.4347 - val_acc: 0.7330\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4604 - acc: 0.7052 - val_loss: 1.4222 - val_acc: 0.7330\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4543 - acc: 0.7083 - val_loss: 1.4165 - val_acc: 0.7360\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4482 - acc: 0.7085 - val_loss: 1.4089 - val_acc: 0.7370\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4433 - acc: 0.7079 - val_loss: 1.4038 - val_acc: 0.7360\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4373 - acc: 0.7072 - val_loss: 1.4000 - val_acc: 0.7440\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4322 - acc: 0.7084 - val_loss: 1.3921 - val_acc: 0.7350\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4265 - acc: 0.7088 - val_loss: 1.3901 - val_acc: 0.7310\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4212 - acc: 0.7091 - val_loss: 1.3863 - val_acc: 0.7380\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4164 - acc: 0.7112 - val_loss: 1.3744 - val_acc: 0.7400\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4107 - acc: 0.7093 - val_loss: 1.3693 - val_acc: 0.7350\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4057 - acc: 0.7113 - val_loss: 1.3632 - val_acc: 0.7450\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4004 - acc: 0.7111 - val_loss: 1.3621 - val_acc: 0.7370\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3956 - acc: 0.7111 - val_loss: 1.3604 - val_acc: 0.7410\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3908 - acc: 0.7117 - val_loss: 1.3533 - val_acc: 0.7370\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3853 - acc: 0.7133 - val_loss: 1.3461 - val_acc: 0.7400\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3812 - acc: 0.7139 - val_loss: 1.3382 - val_acc: 0.7430\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3756 - acc: 0.7132 - val_loss: 1.3367 - val_acc: 0.7440\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3711 - acc: 0.7129 - val_loss: 1.3295 - val_acc: 0.7420\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3661 - acc: 0.7155 - val_loss: 1.3268 - val_acc: 0.7460\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3616 - acc: 0.7140 - val_loss: 1.3228 - val_acc: 0.7350\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3565 - acc: 0.7155 - val_loss: 1.3160 - val_acc: 0.7390\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3522 - acc: 0.7148 - val_loss: 1.3153 - val_acc: 0.7380\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3477 - acc: 0.7151 - val_loss: 1.3073 - val_acc: 0.7400\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3430 - acc: 0.7168 - val_loss: 1.3020 - val_acc: 0.7440\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3384 - acc: 0.7159 - val_loss: 1.3052 - val_acc: 0.7400\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3340 - acc: 0.7181 - val_loss: 1.2931 - val_acc: 0.7450\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3296 - acc: 0.7179 - val_loss: 1.2922 - val_acc: 0.7410\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3256 - acc: 0.7197 - val_loss: 1.2883 - val_acc: 0.7450\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3205 - acc: 0.7193 - val_loss: 1.2826 - val_acc: 0.7470\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3164 - acc: 0.7176 - val_loss: 1.2743 - val_acc: 0.7410\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3119 - acc: 0.7193 - val_loss: 1.2742 - val_acc: 0.7480\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3081 - acc: 0.7200 - val_loss: 1.2755 - val_acc: 0.7430\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3045 - acc: 0.7199 - val_loss: 1.2657 - val_acc: 0.7420\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2996 - acc: 0.7180 - val_loss: 1.2604 - val_acc: 0.7430\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2956 - acc: 0.7203 - val_loss: 1.2612 - val_acc: 0.7490\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2926 - acc: 0.7189 - val_loss: 1.2502 - val_acc: 0.7430\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2875 - acc: 0.7207 - val_loss: 1.2463 - val_acc: 0.7410\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2837 - acc: 0.7205 - val_loss: 1.2449 - val_acc: 0.7420\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2798 - acc: 0.7193 - val_loss: 1.2427 - val_acc: 0.7470\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2759 - acc: 0.7204 - val_loss: 1.2375 - val_acc: 0.7470\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2718 - acc: 0.7211 - val_loss: 1.2319 - val_acc: 0.7520\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2683 - acc: 0.7228 - val_loss: 1.2310 - val_acc: 0.7460\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2649 - acc: 0.7233 - val_loss: 1.2276 - val_acc: 0.7530\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2616 - acc: 0.7223 - val_loss: 1.2226 - val_acc: 0.7450\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2572 - acc: 0.7213 - val_loss: 1.2161 - val_acc: 0.7530\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2535 - acc: 0.7243 - val_loss: 1.2133 - val_acc: 0.7480\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2504 - acc: 0.7239 - val_loss: 1.2137 - val_acc: 0.7500\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2471 - acc: 0.7236 - val_loss: 1.2090 - val_acc: 0.7550\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2437 - acc: 0.7252 - val_loss: 1.2023 - val_acc: 0.7510\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2401 - acc: 0.7259 - val_loss: 1.1987 - val_acc: 0.7520\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2365 - acc: 0.7243 - val_loss: 1.1988 - val_acc: 0.7440\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2339 - acc: 0.7244 - val_loss: 1.1929 - val_acc: 0.7560\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2305 - acc: 0.7263 - val_loss: 1.1913 - val_acc: 0.7540\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2267 - acc: 0.7248 - val_loss: 1.1884 - val_acc: 0.7490\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2241 - acc: 0.7259 - val_loss: 1.1899 - val_acc: 0.7500\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2212 - acc: 0.7244 - val_loss: 1.1802 - val_acc: 0.7520\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2176 - acc: 0.7271 - val_loss: 1.1773 - val_acc: 0.7500\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2145 - acc: 0.7276 - val_loss: 1.1828 - val_acc: 0.7530\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2117 - acc: 0.7273 - val_loss: 1.1738 - val_acc: 0.7570\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2085 - acc: 0.7273 - val_loss: 1.1740 - val_acc: 0.7540\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2059 - acc: 0.7265 - val_loss: 1.1648 - val_acc: 0.7550\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2026 - acc: 0.7275 - val_loss: 1.1691 - val_acc: 0.7570\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2004 - acc: 0.7280 - val_loss: 1.1658 - val_acc: 0.7540\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1972 - acc: 0.7303 - val_loss: 1.1584 - val_acc: 0.7500\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1947 - acc: 0.7295 - val_loss: 1.1558 - val_acc: 0.7580\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1916 - acc: 0.7281 - val_loss: 1.1543 - val_acc: 0.7570\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1891 - acc: 0.7300 - val_loss: 1.1499 - val_acc: 0.7560\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1861 - acc: 0.7300 - val_loss: 1.1513 - val_acc: 0.7570\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1838 - acc: 0.7311 - val_loss: 1.1427 - val_acc: 0.7590\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1815 - acc: 0.7304 - val_loss: 1.1470 - val_acc: 0.7550\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1783 - acc: 0.7321 - val_loss: 1.1461 - val_acc: 0.7560\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1762 - acc: 0.7296 - val_loss: 1.1439 - val_acc: 0.7470\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1740 - acc: 0.7291 - val_loss: 1.1422 - val_acc: 0.7580\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1711 - acc: 0.7321 - val_loss: 1.1356 - val_acc: 0.7560\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1687 - acc: 0.7321 - val_loss: 1.1302 - val_acc: 0.7620\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1659 - acc: 0.7339 - val_loss: 1.1313 - val_acc: 0.7600\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1632 - acc: 0.7348 - val_loss: 1.1299 - val_acc: 0.7590\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1612 - acc: 0.7335 - val_loss: 1.1338 - val_acc: 0.7520\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1595 - acc: 0.7352 - val_loss: 1.1275 - val_acc: 0.7520\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1569 - acc: 0.7344 - val_loss: 1.1311 - val_acc: 0.7520\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1549 - acc: 0.7319 - val_loss: 1.1173 - val_acc: 0.7560\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1524 - acc: 0.7339 - val_loss: 1.1189 - val_acc: 0.7520\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1502 - acc: 0.7331 - val_loss: 1.1125 - val_acc: 0.7580\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1477 - acc: 0.7335 - val_loss: 1.1104 - val_acc: 0.7590\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1459 - acc: 0.7359 - val_loss: 1.1067 - val_acc: 0.7560\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1438 - acc: 0.7364 - val_loss: 1.1083 - val_acc: 0.7580\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1416 - acc: 0.7351 - val_loss: 1.1052 - val_acc: 0.7590\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1396 - acc: 0.7352 - val_loss: 1.1064 - val_acc: 0.7600\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1379 - acc: 0.7353 - val_loss: 1.1007 - val_acc: 0.7590\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1351 - acc: 0.7356 - val_loss: 1.0998 - val_acc: 0.7610\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1340 - acc: 0.7359 - val_loss: 1.0968 - val_acc: 0.7580\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1317 - acc: 0.7363 - val_loss: 1.0995 - val_acc: 0.7540\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1298 - acc: 0.7384 - val_loss: 1.0921 - val_acc: 0.7620\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1278 - acc: 0.7364 - val_loss: 1.0923 - val_acc: 0.7650\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1266 - acc: 0.7365 - val_loss: 1.0886 - val_acc: 0.7620\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1246 - acc: 0.7404 - val_loss: 1.0887 - val_acc: 0.7680\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1230 - acc: 0.7381 - val_loss: 1.0898 - val_acc: 0.7630\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1211 - acc: 0.7371 - val_loss: 1.0855 - val_acc: 0.7580\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1194 - acc: 0.7408 - val_loss: 1.0876 - val_acc: 0.7620\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1177 - acc: 0.7397 - val_loss: 1.0811 - val_acc: 0.7660\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1158 - acc: 0.7391 - val_loss: 1.0888 - val_acc: 0.7610\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1149 - acc: 0.7389 - val_loss: 1.0858 - val_acc: 0.7620\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1134 - acc: 0.7389 - val_loss: 1.0847 - val_acc: 0.7570\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1115 - acc: 0.7411 - val_loss: 1.0830 - val_acc: 0.7580\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1096 - acc: 0.7384 - val_loss: 1.0761 - val_acc: 0.7660\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1083 - acc: 0.7413 - val_loss: 1.0723 - val_acc: 0.7640\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1062 - acc: 0.7412 - val_loss: 1.0771 - val_acc: 0.7610\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1045 - acc: 0.7416 - val_loss: 1.0760 - val_acc: 0.7590\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1034 - acc: 0.7428 - val_loss: 1.0733 - val_acc: 0.7650\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1019 - acc: 0.7407 - val_loss: 1.0684 - val_acc: 0.7590\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0999 - acc: 0.7412 - val_loss: 1.0724 - val_acc: 0.7630\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0991 - acc: 0.7416 - val_loss: 1.0661 - val_acc: 0.7660\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0974 - acc: 0.7435 - val_loss: 1.0613 - val_acc: 0.7680\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0960 - acc: 0.7429 - val_loss: 1.0593 - val_acc: 0.7670\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0942 - acc: 0.7436 - val_loss: 1.0603 - val_acc: 0.7690\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0930 - acc: 0.7433 - val_loss: 1.0582 - val_acc: 0.7640\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0923 - acc: 0.7433 - val_loss: 1.0562 - val_acc: 0.7660\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0900 - acc: 0.7439 - val_loss: 1.0569 - val_acc: 0.7620\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0893 - acc: 0.7461 - val_loss: 1.0531 - val_acc: 0.7690\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0877 - acc: 0.7464 - val_loss: 1.0530 - val_acc: 0.7660\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0860 - acc: 0.7444 - val_loss: 1.0530 - val_acc: 0.7660\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0852 - acc: 0.7456 - val_loss: 1.0525 - val_acc: 0.7690\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0831 - acc: 0.7437 - val_loss: 1.0510 - val_acc: 0.7760\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0822 - acc: 0.7449 - val_loss: 1.0546 - val_acc: 0.7640\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0818 - acc: 0.7449 - val_loss: 1.0498 - val_acc: 0.7680\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0798 - acc: 0.7452 - val_loss: 1.0461 - val_acc: 0.7710\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0780 - acc: 0.7483 - val_loss: 1.0468 - val_acc: 0.7670\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0772 - acc: 0.7469 - val_loss: 1.0474 - val_acc: 0.7640\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0760 - acc: 0.7447 - val_loss: 1.0419 - val_acc: 0.7680\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0745 - acc: 0.7464 - val_loss: 1.0476 - val_acc: 0.7650\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0740 - acc: 0.7461 - val_loss: 1.0441 - val_acc: 0.7690\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0720 - acc: 0.7469 - val_loss: 1.0371 - val_acc: 0.7730\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0708 - acc: 0.7481 - val_loss: 1.0390 - val_acc: 0.7730\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0696 - acc: 0.7489 - val_loss: 1.0363 - val_acc: 0.7680\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0687 - acc: 0.7463 - val_loss: 1.0363 - val_acc: 0.7770\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0673 - acc: 0.7495 - val_loss: 1.0456 - val_acc: 0.7680\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0669 - acc: 0.7477 - val_loss: 1.0413 - val_acc: 0.7620\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0658 - acc: 0.7479 - val_loss: 1.0308 - val_acc: 0.7700\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0640 - acc: 0.7505 - val_loss: 1.0318 - val_acc: 0.7710\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0637 - acc: 0.7495 - val_loss: 1.0321 - val_acc: 0.7680\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0611 - acc: 0.7487 - val_loss: 1.0298 - val_acc: 0.7730\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0604 - acc: 0.7507 - val_loss: 1.0303 - val_acc: 0.7720\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0595 - acc: 0.7504 - val_loss: 1.0324 - val_acc: 0.7720\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0587 - acc: 0.7516 - val_loss: 1.0263 - val_acc: 0.7730\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0575 - acc: 0.7517 - val_loss: 1.0274 - val_acc: 0.7720\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0563 - acc: 0.7513 - val_loss: 1.0227 - val_acc: 0.7730\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0550 - acc: 0.7513 - val_loss: 1.0211 - val_acc: 0.7680\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0547 - acc: 0.7516 - val_loss: 1.0234 - val_acc: 0.7710\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0530 - acc: 0.7511 - val_loss: 1.0233 - val_acc: 0.7700\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0520 - acc: 0.7549 - val_loss: 1.0211 - val_acc: 0.7700\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0505 - acc: 0.7513 - val_loss: 1.0217 - val_acc: 0.7720\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0497 - acc: 0.7523 - val_loss: 1.0293 - val_acc: 0.7670\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0499 - acc: 0.7507 - val_loss: 1.0219 - val_acc: 0.7710\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0481 - acc: 0.7517 - val_loss: 1.0180 - val_acc: 0.7730\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0466 - acc: 0.7540 - val_loss: 1.0231 - val_acc: 0.7700\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0462 - acc: 0.7536 - val_loss: 1.0189 - val_acc: 0.7770\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0446 - acc: 0.7543 - val_loss: 1.0184 - val_acc: 0.7710\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0434 - acc: 0.7536 - val_loss: 1.0134 - val_acc: 0.7700\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0419 - acc: 0.7549 - val_loss: 1.0170 - val_acc: 0.7690\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0412 - acc: 0.7549 - val_loss: 1.0196 - val_acc: 0.7770\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0409 - acc: 0.7505 - val_loss: 1.0184 - val_acc: 0.7630\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0396 - acc: 0.7552 - val_loss: 1.0095 - val_acc: 0.7740\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0385 - acc: 0.7565 - val_loss: 1.0109 - val_acc: 0.7740\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0378 - acc: 0.7540 - val_loss: 1.0101 - val_acc: 0.7740\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0362 - acc: 0.7553 - val_loss: 1.0089 - val_acc: 0.7720\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0349 - acc: 0.7567 - val_loss: 1.0102 - val_acc: 0.7720\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0348 - acc: 0.7559 - val_loss: 1.0079 - val_acc: 0.7720\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0328 - acc: 0.7569 - val_loss: 1.0086 - val_acc: 0.7720\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0329 - acc: 0.7543 - val_loss: 1.0285 - val_acc: 0.7650\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0330 - acc: 0.7548 - val_loss: 1.0052 - val_acc: 0.7740\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0304 - acc: 0.7573 - val_loss: 1.0063 - val_acc: 0.7780\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0291 - acc: 0.7545 - val_loss: 1.0003 - val_acc: 0.7780\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0280 - acc: 0.7568 - val_loss: 1.0031 - val_acc: 0.7800\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0281 - acc: 0.7583 - val_loss: 0.9995 - val_acc: 0.7780\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0265 - acc: 0.7569 - val_loss: 1.0038 - val_acc: 0.7730\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0257 - acc: 0.7559 - val_loss: 1.0016 - val_acc: 0.7770\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0246 - acc: 0.7555 - val_loss: 1.0022 - val_acc: 0.7790\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0241 - acc: 0.7591 - val_loss: 0.9951 - val_acc: 0.7770\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0226 - acc: 0.7601 - val_loss: 0.9994 - val_acc: 0.7810\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0225 - acc: 0.7585 - val_loss: 0.9966 - val_acc: 0.7730\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0213 - acc: 0.7584 - val_loss: 0.9944 - val_acc: 0.7750\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0203 - acc: 0.7584 - val_loss: 0.9941 - val_acc: 0.7760\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0197 - acc: 0.7575 - val_loss: 0.9957 - val_acc: 0.7750\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0189 - acc: 0.7581 - val_loss: 0.9936 - val_acc: 0.7790\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0169 - acc: 0.7601 - val_loss: 0.9894 - val_acc: 0.7740\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0171 - acc: 0.7577 - val_loss: 0.9978 - val_acc: 0.7790\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0166 - acc: 0.7587 - val_loss: 0.9925 - val_acc: 0.7800\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0151 - acc: 0.7591 - val_loss: 0.9862 - val_acc: 0.7760\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0142 - acc: 0.7599 - val_loss: 0.9951 - val_acc: 0.7770\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0132 - acc: 0.7603 - val_loss: 0.9879 - val_acc: 0.7740\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0124 - acc: 0.7603 - val_loss: 0.9865 - val_acc: 0.7790\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0119 - acc: 0.7595 - val_loss: 0.9845 - val_acc: 0.7750\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0111 - acc: 0.7607 - val_loss: 0.9930 - val_acc: 0.7740\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0104 - acc: 0.7597 - val_loss: 0.9844 - val_acc: 0.7790\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0090 - acc: 0.7617 - val_loss: 0.9888 - val_acc: 0.7810\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0082 - acc: 0.7585 - val_loss: 0.9859 - val_acc: 0.7830\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0075 - acc: 0.7615 - val_loss: 0.9852 - val_acc: 0.7820\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0070 - acc: 0.7596 - val_loss: 0.9809 - val_acc: 0.7750\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0059 - acc: 0.7613 - val_loss: 0.9853 - val_acc: 0.7770\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0060 - acc: 0.7603 - val_loss: 0.9828 - val_acc: 0.7790\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0040 - acc: 0.7596 - val_loss: 0.9802 - val_acc: 0.7780\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0039 - acc: 0.7608 - val_loss: 0.9753 - val_acc: 0.7800\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0026 - acc: 0.7623 - val_loss: 0.9811 - val_acc: 0.7760\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0012 - acc: 0.7629 - val_loss: 0.9823 - val_acc: 0.7800\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0008 - acc: 0.7613 - val_loss: 0.9774 - val_acc: 0.7790\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0008 - acc: 0.7631 - val_loss: 0.9737 - val_acc: 0.7770\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9996 - acc: 0.7617 - val_loss: 0.9758 - val_acc: 0.7800\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9988 - acc: 0.7627 - val_loss: 0.9790 - val_acc: 0.7840\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9988 - acc: 0.7623 - val_loss: 0.9793 - val_acc: 0.7830\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9979 - acc: 0.7649 - val_loss: 0.9784 - val_acc: 0.7760\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9956 - acc: 0.7652 - val_loss: 0.9788 - val_acc: 0.7810\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9964 - acc: 0.7635 - val_loss: 0.9837 - val_acc: 0.7760\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9956 - acc: 0.7616 - val_loss: 0.9812 - val_acc: 0.7800\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9951 - acc: 0.7635 - val_loss: 0.9790 - val_acc: 0.7830\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9933 - acc: 0.7645 - val_loss: 0.9692 - val_acc: 0.7860\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9929 - acc: 0.7639 - val_loss: 0.9740 - val_acc: 0.7880\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9928 - acc: 0.7639 - val_loss: 0.9748 - val_acc: 0.7800\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9925 - acc: 0.7627 - val_loss: 0.9722 - val_acc: 0.7730\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9901 - acc: 0.7643 - val_loss: 0.9744 - val_acc: 0.7720\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9904 - acc: 0.7657 - val_loss: 0.9666 - val_acc: 0.7870\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9886 - acc: 0.7649 - val_loss: 0.9684 - val_acc: 0.7840\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9884 - acc: 0.7660 - val_loss: 0.9688 - val_acc: 0.7820\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9875 - acc: 0.7655 - val_loss: 0.9634 - val_acc: 0.7850\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9865 - acc: 0.7661 - val_loss: 0.9675 - val_acc: 0.7830\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9863 - acc: 0.7639 - val_loss: 0.9612 - val_acc: 0.7810\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9856 - acc: 0.7643 - val_loss: 0.9654 - val_acc: 0.7830\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9852 - acc: 0.7640 - val_loss: 0.9680 - val_acc: 0.7780\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9839 - acc: 0.7660 - val_loss: 0.9614 - val_acc: 0.7840\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9835 - acc: 0.7631 - val_loss: 0.9653 - val_acc: 0.7820\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9824 - acc: 0.7651 - val_loss: 0.9673 - val_acc: 0.7800\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9818 - acc: 0.7655 - val_loss: 0.9593 - val_acc: 0.7840\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9815 - acc: 0.7641 - val_loss: 0.9614 - val_acc: 0.7830\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9804 - acc: 0.7667 - val_loss: 0.9602 - val_acc: 0.7880\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9801 - acc: 0.7660 - val_loss: 0.9579 - val_acc: 0.7850\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9804 - acc: 0.7667 - val_loss: 0.9596 - val_acc: 0.7790\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9793 - acc: 0.7651 - val_loss: 0.9549 - val_acc: 0.7820\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9780 - acc: 0.7665 - val_loss: 0.9587 - val_acc: 0.7830\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9772 - acc: 0.7695 - val_loss: 0.9714 - val_acc: 0.7800\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9779 - acc: 0.7657 - val_loss: 0.9573 - val_acc: 0.7830\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9754 - acc: 0.7673 - val_loss: 0.9649 - val_acc: 0.7790\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9757 - acc: 0.7653 - val_loss: 0.9572 - val_acc: 0.7830\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9746 - acc: 0.7655 - val_loss: 0.9539 - val_acc: 0.7830\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9741 - acc: 0.7692 - val_loss: 0.9591 - val_acc: 0.7760\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9738 - acc: 0.7648 - val_loss: 0.9597 - val_acc: 0.7800\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9728 - acc: 0.7669 - val_loss: 0.9532 - val_acc: 0.7870\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9722 - acc: 0.7667 - val_loss: 0.9543 - val_acc: 0.7910\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9722 - acc: 0.7684 - val_loss: 0.9585 - val_acc: 0.7820\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9711 - acc: 0.7665 - val_loss: 0.9574 - val_acc: 0.7870\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9704 - acc: 0.7667 - val_loss: 0.9587 - val_acc: 0.7820\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9698 - acc: 0.7685 - val_loss: 0.9519 - val_acc: 0.7860\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9685 - acc: 0.7697 - val_loss: 0.9562 - val_acc: 0.7820\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9690 - acc: 0.7684 - val_loss: 0.9489 - val_acc: 0.7870\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9675 - acc: 0.7677 - val_loss: 0.9586 - val_acc: 0.7810\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9663 - acc: 0.7689 - val_loss: 0.9533 - val_acc: 0.7840\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9669 - acc: 0.7679 - val_loss: 0.9539 - val_acc: 0.7860\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9654 - acc: 0.7700 - val_loss: 0.9489 - val_acc: 0.7820\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9658 - acc: 0.7688 - val_loss: 0.9509 - val_acc: 0.7800\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9649 - acc: 0.7695 - val_loss: 0.9499 - val_acc: 0.7840\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9636 - acc: 0.7689 - val_loss: 0.9598 - val_acc: 0.7800\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9636 - acc: 0.7684 - val_loss: 0.9589 - val_acc: 0.7810\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9630 - acc: 0.7707 - val_loss: 0.9477 - val_acc: 0.7810\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9618 - acc: 0.7727 - val_loss: 0.9421 - val_acc: 0.7850\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9611 - acc: 0.7692 - val_loss: 0.9460 - val_acc: 0.7840\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9610 - acc: 0.7699 - val_loss: 0.9488 - val_acc: 0.7840\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9615 - acc: 0.7733 - val_loss: 0.9446 - val_acc: 0.7880\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9590 - acc: 0.7711 - val_loss: 0.9470 - val_acc: 0.7880\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9601 - acc: 0.7712 - val_loss: 0.9435 - val_acc: 0.7860\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9583 - acc: 0.7701 - val_loss: 0.9420 - val_acc: 0.7840\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9583 - acc: 0.7697 - val_loss: 0.9425 - val_acc: 0.7860\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9574 - acc: 0.7699 - val_loss: 0.9427 - val_acc: 0.7840\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9576 - acc: 0.7695 - val_loss: 0.9407 - val_acc: 0.7870\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9548 - acc: 0.7733 - val_loss: 0.9438 - val_acc: 0.7820\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9556 - acc: 0.7711 - val_loss: 0.9468 - val_acc: 0.7820\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9548 - acc: 0.7724 - val_loss: 0.9425 - val_acc: 0.7880\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9542 - acc: 0.7712 - val_loss: 0.9435 - val_acc: 0.7900\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9531 - acc: 0.7745 - val_loss: 0.9463 - val_acc: 0.7820\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9541 - acc: 0.7707 - val_loss: 0.9432 - val_acc: 0.7890\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9525 - acc: 0.7717 - val_loss: 0.9559 - val_acc: 0.7830\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9521 - acc: 0.7728 - val_loss: 0.9380 - val_acc: 0.7820\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9512 - acc: 0.7725 - val_loss: 0.9356 - val_acc: 0.7850\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9512 - acc: 0.7729 - val_loss: 0.9367 - val_acc: 0.7880\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9505 - acc: 0.7719 - val_loss: 0.9344 - val_acc: 0.7870\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9489 - acc: 0.7731 - val_loss: 0.9369 - val_acc: 0.7890\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9493 - acc: 0.7728 - val_loss: 0.9377 - val_acc: 0.7890\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9485 - acc: 0.7736 - val_loss: 0.9390 - val_acc: 0.7860\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9486 - acc: 0.7735 - val_loss: 0.9448 - val_acc: 0.7840\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9480 - acc: 0.7744 - val_loss: 0.9515 - val_acc: 0.7800\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9477 - acc: 0.7749 - val_loss: 0.9367 - val_acc: 0.7880\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9467 - acc: 0.7723 - val_loss: 0.9376 - val_acc: 0.7830\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9458 - acc: 0.7735 - val_loss: 0.9372 - val_acc: 0.7840\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9460 - acc: 0.7749 - val_loss: 0.9356 - val_acc: 0.7840\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9453 - acc: 0.7732 - val_loss: 0.9321 - val_acc: 0.7880\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9447 - acc: 0.7751 - val_loss: 0.9353 - val_acc: 0.7860\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9446 - acc: 0.7753 - val_loss: 0.9286 - val_acc: 0.7880\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9439 - acc: 0.7751 - val_loss: 0.9342 - val_acc: 0.7850\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9433 - acc: 0.7740 - val_loss: 0.9357 - val_acc: 0.7880\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9425 - acc: 0.7744 - val_loss: 0.9406 - val_acc: 0.7850\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9426 - acc: 0.7761 - val_loss: 0.9272 - val_acc: 0.7870\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9412 - acc: 0.7781 - val_loss: 0.9311 - val_acc: 0.7910\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9401 - acc: 0.7767 - val_loss: 0.9312 - val_acc: 0.7840\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9410 - acc: 0.7740 - val_loss: 0.9258 - val_acc: 0.7900\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9400 - acc: 0.7748 - val_loss: 0.9289 - val_acc: 0.7870\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9393 - acc: 0.7787 - val_loss: 0.9254 - val_acc: 0.7840\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9393 - acc: 0.7764 - val_loss: 0.9424 - val_acc: 0.7860\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9381 - acc: 0.7771 - val_loss: 0.9252 - val_acc: 0.7870\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9386 - acc: 0.7772 - val_loss: 0.9239 - val_acc: 0.7880\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9373 - acc: 0.7773 - val_loss: 0.9285 - val_acc: 0.7910\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9376 - acc: 0.7775 - val_loss: 0.9405 - val_acc: 0.7840\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9385 - acc: 0.7779 - val_loss: 0.9277 - val_acc: 0.7920\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9371 - acc: 0.7781 - val_loss: 0.9295 - val_acc: 0.7910\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9363 - acc: 0.7767 - val_loss: 0.9315 - val_acc: 0.7790\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9353 - acc: 0.7761 - val_loss: 0.9228 - val_acc: 0.7850\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9353 - acc: 0.7780 - val_loss: 0.9253 - val_acc: 0.7830\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9349 - acc: 0.7773 - val_loss: 0.9297 - val_acc: 0.7900\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9344 - acc: 0.7785 - val_loss: 0.9209 - val_acc: 0.7890\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9336 - acc: 0.7768 - val_loss: 0.9322 - val_acc: 0.7920\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9342 - acc: 0.7792 - val_loss: 0.9238 - val_acc: 0.7920\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9322 - acc: 0.7781 - val_loss: 0.9252 - val_acc: 0.7870\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9323 - acc: 0.7777 - val_loss: 0.9190 - val_acc: 0.7860\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9317 - acc: 0.7753 - val_loss: 0.9283 - val_acc: 0.7850\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9321 - acc: 0.7803 - val_loss: 0.9285 - val_acc: 0.7880\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9310 - acc: 0.7800 - val_loss: 0.9179 - val_acc: 0.7890\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9307 - acc: 0.7784 - val_loss: 0.9200 - val_acc: 0.7890\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9294 - acc: 0.7775 - val_loss: 0.9233 - val_acc: 0.7830\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9301 - acc: 0.7783 - val_loss: 0.9229 - val_acc: 0.7910\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9297 - acc: 0.7812 - val_loss: 0.9185 - val_acc: 0.7850\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9292 - acc: 0.7796 - val_loss: 0.9294 - val_acc: 0.7880\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9281 - acc: 0.7808 - val_loss: 0.9299 - val_acc: 0.7810\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9281 - acc: 0.7815 - val_loss: 0.9221 - val_acc: 0.7910\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9276 - acc: 0.7812 - val_loss: 0.9194 - val_acc: 0.7880\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9273 - acc: 0.7821 - val_loss: 0.9155 - val_acc: 0.7900\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9259 - acc: 0.7811 - val_loss: 0.9168 - val_acc: 0.7940\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7813 - val_loss: 0.9222 - val_acc: 0.7910\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9259 - acc: 0.7803 - val_loss: 0.9330 - val_acc: 0.7810\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9260 - acc: 0.7808 - val_loss: 0.9286 - val_acc: 0.7890\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9261 - acc: 0.7801 - val_loss: 0.9152 - val_acc: 0.7910\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9258 - acc: 0.7792 - val_loss: 0.9197 - val_acc: 0.7980\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9239 - acc: 0.7821 - val_loss: 0.9140 - val_acc: 0.7910\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9254 - acc: 0.7800 - val_loss: 0.9155 - val_acc: 0.7950\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9246 - acc: 0.7801 - val_loss: 0.9154 - val_acc: 0.7920\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9231 - acc: 0.7811 - val_loss: 0.9203 - val_acc: 0.7950\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9230 - acc: 0.7808 - val_loss: 0.9193 - val_acc: 0.7930\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9228 - acc: 0.7817 - val_loss: 0.9178 - val_acc: 0.7880\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9215 - acc: 0.7819 - val_loss: 0.9124 - val_acc: 0.7910\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9221 - acc: 0.7807 - val_loss: 0.9199 - val_acc: 0.7850\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9220 - acc: 0.7828 - val_loss: 0.9123 - val_acc: 0.8010\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9204 - acc: 0.7837 - val_loss: 0.9129 - val_acc: 0.7910\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9202 - acc: 0.7829 - val_loss: 0.9132 - val_acc: 0.7950\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9194 - acc: 0.7807 - val_loss: 0.9137 - val_acc: 0.7920\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9201 - acc: 0.7817 - val_loss: 0.9114 - val_acc: 0.7860\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9199 - acc: 0.7843 - val_loss: 0.9172 - val_acc: 0.7900\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9194 - acc: 0.7852 - val_loss: 0.9095 - val_acc: 0.7860\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9184 - acc: 0.7847 - val_loss: 0.9114 - val_acc: 0.7910\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9173 - acc: 0.7843 - val_loss: 0.9145 - val_acc: 0.7900\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9175 - acc: 0.7839 - val_loss: 0.9213 - val_acc: 0.7870\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9176 - acc: 0.7815 - val_loss: 0.9146 - val_acc: 0.7920\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9169 - acc: 0.7823 - val_loss: 0.9121 - val_acc: 0.7970\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9167 - acc: 0.7848 - val_loss: 0.9424 - val_acc: 0.7820\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9168 - acc: 0.7833 - val_loss: 0.9129 - val_acc: 0.7900\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9159 - acc: 0.7831 - val_loss: 0.9118 - val_acc: 0.7980\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9161 - acc: 0.7847 - val_loss: 0.9135 - val_acc: 0.7950\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9156 - acc: 0.7828 - val_loss: 0.9156 - val_acc: 0.7900\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9143 - acc: 0.7840 - val_loss: 0.9071 - val_acc: 0.7960\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9142 - acc: 0.7852 - val_loss: 0.9097 - val_acc: 0.7980\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9138 - acc: 0.7828 - val_loss: 0.9128 - val_acc: 0.7910\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9134 - acc: 0.7841 - val_loss: 0.9239 - val_acc: 0.7970\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9137 - acc: 0.7872 - val_loss: 0.9087 - val_acc: 0.7930\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9130 - acc: 0.7841 - val_loss: 0.9076 - val_acc: 0.7980\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9135 - acc: 0.7865 - val_loss: 0.9174 - val_acc: 0.7890\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9126 - acc: 0.7848 - val_loss: 0.9098 - val_acc: 0.7960\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9124 - acc: 0.7857 - val_loss: 0.9216 - val_acc: 0.7880\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9117 - acc: 0.7856 - val_loss: 0.9042 - val_acc: 0.7950\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9109 - acc: 0.7844 - val_loss: 0.9074 - val_acc: 0.7980\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9107 - acc: 0.7861 - val_loss: 0.9111 - val_acc: 0.7960\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9102 - acc: 0.7855 - val_loss: 0.9123 - val_acc: 0.7940\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9100 - acc: 0.7876 - val_loss: 0.9083 - val_acc: 0.7910\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9101 - acc: 0.7864 - val_loss: 0.9085 - val_acc: 0.7950\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9100 - acc: 0.7851 - val_loss: 0.9101 - val_acc: 0.8000\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9092 - acc: 0.7875 - val_loss: 0.9072 - val_acc: 0.7930\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9089 - acc: 0.7867 - val_loss: 0.9035 - val_acc: 0.7890\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9080 - acc: 0.7865 - val_loss: 0.9025 - val_acc: 0.7960\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9088 - acc: 0.7863 - val_loss: 0.9042 - val_acc: 0.7950\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9084 - acc: 0.7873 - val_loss: 0.9048 - val_acc: 0.7960\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9085 - acc: 0.7855 - val_loss: 0.9042 - val_acc: 0.7930\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9080 - acc: 0.7881 - val_loss: 0.9174 - val_acc: 0.7870\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9074 - acc: 0.7875 - val_loss: 0.9273 - val_acc: 0.7810\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9076 - acc: 0.7864 - val_loss: 0.9071 - val_acc: 0.7900\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9058 - acc: 0.7877 - val_loss: 0.9016 - val_acc: 0.7970\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9061 - acc: 0.7872 - val_loss: 0.9073 - val_acc: 0.7950\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9055 - acc: 0.7876 - val_loss: 0.9036 - val_acc: 0.8000\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9058 - acc: 0.7900 - val_loss: 0.9119 - val_acc: 0.7920\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9047 - acc: 0.7865 - val_loss: 0.9024 - val_acc: 0.7980\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9052 - acc: 0.7905 - val_loss: 0.9127 - val_acc: 0.7910\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9065 - acc: 0.7863 - val_loss: 0.9065 - val_acc: 0.7940\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9039 - acc: 0.7884 - val_loss: 0.9109 - val_acc: 0.7890\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9050 - acc: 0.7881 - val_loss: 0.9010 - val_acc: 0.7950\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9042 - acc: 0.7912 - val_loss: 0.9010 - val_acc: 0.7970\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9028 - acc: 0.7908 - val_loss: 0.9031 - val_acc: 0.7960\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9030 - acc: 0.7909 - val_loss: 0.8993 - val_acc: 0.7990\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9030 - acc: 0.7891 - val_loss: 0.9105 - val_acc: 0.8000\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9041 - acc: 0.7895 - val_loss: 0.9143 - val_acc: 0.7950\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9026 - acc: 0.7889 - val_loss: 0.9060 - val_acc: 0.7930\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9024 - acc: 0.7883 - val_loss: 0.9187 - val_acc: 0.7880\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9022 - acc: 0.7912 - val_loss: 0.9035 - val_acc: 0.7890\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9027 - acc: 0.7899 - val_loss: 0.9296 - val_acc: 0.7770\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9028 - acc: 0.7884 - val_loss: 0.9132 - val_acc: 0.7920\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9021 - acc: 0.7892 - val_loss: 0.9020 - val_acc: 0.8020\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9002 - acc: 0.7883 - val_loss: 0.8986 - val_acc: 0.7940\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9017 - acc: 0.7893 - val_loss: 0.8981 - val_acc: 0.7950\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8988 - acc: 0.7917 - val_loss: 0.9087 - val_acc: 0.8020\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9008 - acc: 0.7879 - val_loss: 0.8980 - val_acc: 0.8010\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9011 - acc: 0.7892 - val_loss: 0.9097 - val_acc: 0.8010\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8987 - acc: 0.7899 - val_loss: 0.9033 - val_acc: 0.7960\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8987 - acc: 0.7889 - val_loss: 0.9008 - val_acc: 0.7970\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8990 - acc: 0.7907 - val_loss: 0.8995 - val_acc: 0.8020\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8985 - acc: 0.7900 - val_loss: 0.9009 - val_acc: 0.8040\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8984 - acc: 0.7903 - val_loss: 0.9097 - val_acc: 0.8000\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8986 - acc: 0.7900 - val_loss: 0.9078 - val_acc: 0.7940\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8989 - acc: 0.7896 - val_loss: 0.8969 - val_acc: 0.7940\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8979 - acc: 0.7896 - val_loss: 0.9011 - val_acc: 0.7990\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8967 - acc: 0.7896 - val_loss: 0.9010 - val_acc: 0.7970\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8964 - acc: 0.7908 - val_loss: 0.8999 - val_acc: 0.8000\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8974 - acc: 0.7889 - val_loss: 0.9004 - val_acc: 0.8000\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8972 - acc: 0.7895 - val_loss: 0.9012 - val_acc: 0.8000\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8964 - acc: 0.7875 - val_loss: 0.8941 - val_acc: 0.7960\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8959 - acc: 0.7917 - val_loss: 0.8977 - val_acc: 0.7950\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8960 - acc: 0.7912 - val_loss: 0.8981 - val_acc: 0.7970\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7928 - val_loss: 0.8953 - val_acc: 0.7990\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8938 - acc: 0.7913 - val_loss: 0.9097 - val_acc: 0.7970\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8961 - acc: 0.7888 - val_loss: 0.8995 - val_acc: 0.7920\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8943 - acc: 0.7901 - val_loss: 0.8969 - val_acc: 0.7980\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8940 - acc: 0.7915 - val_loss: 0.8990 - val_acc: 0.8000\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8944 - acc: 0.7931 - val_loss: 0.9045 - val_acc: 0.7960\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8939 - acc: 0.7935 - val_loss: 0.8990 - val_acc: 0.7910\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8953 - acc: 0.7901 - val_loss: 0.8969 - val_acc: 0.8000\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8940 - acc: 0.7919 - val_loss: 0.8946 - val_acc: 0.7970\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8932 - acc: 0.7915 - val_loss: 0.9234 - val_acc: 0.7890\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8929 - acc: 0.7928 - val_loss: 0.8935 - val_acc: 0.7950\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8935 - acc: 0.7925 - val_loss: 0.9092 - val_acc: 0.7920\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8933 - acc: 0.7907 - val_loss: 0.8959 - val_acc: 0.7960\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8919 - acc: 0.7949 - val_loss: 0.9067 - val_acc: 0.8000\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8917 - acc: 0.7916 - val_loss: 0.8953 - val_acc: 0.7990\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8904 - acc: 0.7949 - val_loss: 0.8942 - val_acc: 0.8000\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8906 - acc: 0.7947 - val_loss: 0.9011 - val_acc: 0.8040\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8914 - acc: 0.7925 - val_loss: 0.9046 - val_acc: 0.8000\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8906 - acc: 0.7912 - val_loss: 0.8918 - val_acc: 0.7960\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8901 - acc: 0.7927 - val_loss: 0.8952 - val_acc: 0.7970\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8896 - acc: 0.7964 - val_loss: 0.8897 - val_acc: 0.7960\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8889 - acc: 0.7933 - val_loss: 0.8978 - val_acc: 0.7970\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8901 - acc: 0.7932 - val_loss: 0.9071 - val_acc: 0.8010\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8903 - acc: 0.7937 - val_loss: 0.9052 - val_acc: 0.7990\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8906 - acc: 0.7928 - val_loss: 0.9027 - val_acc: 0.8010\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8893 - acc: 0.7927 - val_loss: 0.8986 - val_acc: 0.7980\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8889 - acc: 0.7932 - val_loss: 0.9117 - val_acc: 0.7990\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8887 - acc: 0.7941 - val_loss: 0.8979 - val_acc: 0.8040\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8883 - acc: 0.7945 - val_loss: 0.8903 - val_acc: 0.7970\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8882 - acc: 0.7949 - val_loss: 0.8916 - val_acc: 0.7960\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8871 - acc: 0.7949 - val_loss: 0.8974 - val_acc: 0.8060\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8880 - acc: 0.7933 - val_loss: 0.8952 - val_acc: 0.8040\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8872 - acc: 0.7953 - val_loss: 0.8999 - val_acc: 0.8010\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8868 - acc: 0.7961 - val_loss: 0.8948 - val_acc: 0.8040\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8877 - acc: 0.7937 - val_loss: 0.8908 - val_acc: 0.8050\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8872 - acc: 0.7939 - val_loss: 0.8878 - val_acc: 0.8010\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8862 - acc: 0.7933 - val_loss: 0.8894 - val_acc: 0.8000\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8858 - acc: 0.7948 - val_loss: 0.8901 - val_acc: 0.7950\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8855 - acc: 0.7951 - val_loss: 0.9008 - val_acc: 0.8010\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8851 - acc: 0.7967 - val_loss: 0.9036 - val_acc: 0.8010\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8860 - acc: 0.7933 - val_loss: 0.9058 - val_acc: 0.8030\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8860 - acc: 0.7928 - val_loss: 0.8935 - val_acc: 0.8030\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8843 - acc: 0.7967 - val_loss: 0.8862 - val_acc: 0.7980\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8847 - acc: 0.7957 - val_loss: 0.8884 - val_acc: 0.7940\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8854 - acc: 0.7949 - val_loss: 0.8899 - val_acc: 0.8020\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8840 - acc: 0.7979 - val_loss: 0.8946 - val_acc: 0.7930\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8843 - acc: 0.7961 - val_loss: 0.8894 - val_acc: 0.7950\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8835 - acc: 0.7953 - val_loss: 0.8976 - val_acc: 0.7940\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8847 - acc: 0.7967 - val_loss: 0.9031 - val_acc: 0.8030\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8825 - acc: 0.7959 - val_loss: 0.8872 - val_acc: 0.8020\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8822 - acc: 0.7963 - val_loss: 0.8931 - val_acc: 0.8080\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8820 - acc: 0.7953 - val_loss: 0.8880 - val_acc: 0.8000\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8816 - acc: 0.7968 - val_loss: 0.8881 - val_acc: 0.8020\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8825 - acc: 0.7923 - val_loss: 0.8874 - val_acc: 0.7960\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8832 - acc: 0.7977 - val_loss: 0.8901 - val_acc: 0.7990\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8822 - acc: 0.7979 - val_loss: 0.8880 - val_acc: 0.8010\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8813 - acc: 0.7976 - val_loss: 0.8899 - val_acc: 0.8000\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8820 - acc: 0.7981 - val_loss: 0.8901 - val_acc: 0.7980\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8811 - acc: 0.7969 - val_loss: 0.8925 - val_acc: 0.8060\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8808 - acc: 0.7983 - val_loss: 0.8959 - val_acc: 0.8080\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8801 - acc: 0.7965 - val_loss: 0.8874 - val_acc: 0.8040\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8793 - acc: 0.7988 - val_loss: 0.8981 - val_acc: 0.8010\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8811 - acc: 0.7963 - val_loss: 0.8900 - val_acc: 0.8010\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8794 - acc: 0.7973 - val_loss: 0.8914 - val_acc: 0.8070\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8803 - acc: 0.7968 - val_loss: 0.8854 - val_acc: 0.8000\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8801 - acc: 0.7989 - val_loss: 0.8861 - val_acc: 0.8040\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8784 - acc: 0.7975 - val_loss: 0.8884 - val_acc: 0.8070\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8784 - acc: 0.7988 - val_loss: 0.8997 - val_acc: 0.7930\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8797 - acc: 0.7981 - val_loss: 0.9017 - val_acc: 0.7990\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8787 - acc: 0.7977 - val_loss: 0.8915 - val_acc: 0.8050\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8791 - acc: 0.7983 - val_loss: 0.8903 - val_acc: 0.8070\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8801 - acc: 0.7981 - val_loss: 0.8871 - val_acc: 0.7990\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8782 - acc: 0.8000 - val_loss: 0.8857 - val_acc: 0.8020\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8784 - acc: 0.7993 - val_loss: 0.8889 - val_acc: 0.8040\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8770 - acc: 0.7976 - val_loss: 0.8984 - val_acc: 0.8080\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8782 - acc: 0.8000 - val_loss: 0.9014 - val_acc: 0.8000\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8774 - acc: 0.7960 - val_loss: 0.9000 - val_acc: 0.7920\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8776 - acc: 0.7965 - val_loss: 0.8900 - val_acc: 0.8040\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8760 - acc: 0.7987 - val_loss: 0.8851 - val_acc: 0.8020\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8760 - acc: 0.7999 - val_loss: 0.8824 - val_acc: 0.7990\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8760 - acc: 0.7984 - val_loss: 0.8894 - val_acc: 0.8110\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8751 - acc: 0.7975 - val_loss: 0.8950 - val_acc: 0.8020\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8765 - acc: 0.7979 - val_loss: 0.8874 - val_acc: 0.8070\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8744 - acc: 0.7999 - val_loss: 0.8875 - val_acc: 0.8020\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8756 - acc: 0.8000 - val_loss: 0.9137 - val_acc: 0.7900\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8764 - acc: 0.7981 - val_loss: 0.8813 - val_acc: 0.8010\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8737 - acc: 0.7993 - val_loss: 0.9135 - val_acc: 0.7980\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8747 - acc: 0.7991 - val_loss: 0.9064 - val_acc: 0.7950\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8742 - acc: 0.7979 - val_loss: 0.8900 - val_acc: 0.7950\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8741 - acc: 0.7993 - val_loss: 0.8886 - val_acc: 0.7940\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8744 - acc: 0.7989 - val_loss: 0.8897 - val_acc: 0.8090\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8733 - acc: 0.7976 - val_loss: 0.8978 - val_acc: 0.7950\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8732 - acc: 0.7992 - val_loss: 0.8943 - val_acc: 0.8070\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8720 - acc: 0.7991 - val_loss: 0.9049 - val_acc: 0.7890\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8734 - acc: 0.7973 - val_loss: 0.9010 - val_acc: 0.7950\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8711 - acc: 0.7987 - val_loss: 0.8847 - val_acc: 0.8020\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8715 - acc: 0.8001 - val_loss: 0.8844 - val_acc: 0.7990\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8727 - acc: 0.8003 - val_loss: 0.8830 - val_acc: 0.8030\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8723 - acc: 0.7991 - val_loss: 0.8847 - val_acc: 0.8070\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8723 - acc: 0.7973 - val_loss: 0.8835 - val_acc: 0.8000\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8711 - acc: 0.7989 - val_loss: 0.8793 - val_acc: 0.7950\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8709 - acc: 0.8027 - val_loss: 0.8810 - val_acc: 0.8030\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8705 - acc: 0.8005 - val_loss: 0.8822 - val_acc: 0.7990\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8719 - acc: 0.8007 - val_loss: 0.8953 - val_acc: 0.7920\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8713 - acc: 0.7985 - val_loss: 0.8790 - val_acc: 0.8010\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8707 - acc: 0.8000 - val_loss: 0.8882 - val_acc: 0.8090\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8704 - acc: 0.7997 - val_loss: 0.8874 - val_acc: 0.8040\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8692 - acc: 0.8005 - val_loss: 0.8854 - val_acc: 0.7980\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8710 - acc: 0.7999 - val_loss: 0.8825 - val_acc: 0.8040\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8705 - acc: 0.8004 - val_loss: 0.8828 - val_acc: 0.8110\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8698 - acc: 0.8015 - val_loss: 0.8780 - val_acc: 0.8030\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8686 - acc: 0.8015 - val_loss: 0.8829 - val_acc: 0.7980\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8699 - acc: 0.8013 - val_loss: 0.8803 - val_acc: 0.8010\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8677 - acc: 0.8035 - val_loss: 0.8793 - val_acc: 0.8030\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8668 - acc: 0.8025 - val_loss: 0.8938 - val_acc: 0.8020\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8677 - acc: 0.8015 - val_loss: 0.8810 - val_acc: 0.8060\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8683 - acc: 0.7996 - val_loss: 0.8866 - val_acc: 0.8130\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8665 - acc: 0.8015 - val_loss: 0.8850 - val_acc: 0.8060\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8676 - acc: 0.8008 - val_loss: 0.8860 - val_acc: 0.7930\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8672 - acc: 0.8023 - val_loss: 0.8857 - val_acc: 0.8000\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8668 - acc: 0.7997 - val_loss: 0.8790 - val_acc: 0.8030\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8681 - acc: 0.8012 - val_loss: 0.8865 - val_acc: 0.8130\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8681 - acc: 0.8017 - val_loss: 0.8760 - val_acc: 0.8100\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8656 - acc: 0.8037 - val_loss: 0.8906 - val_acc: 0.7970\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8659 - acc: 0.8023 - val_loss: 0.8775 - val_acc: 0.7960\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8663 - acc: 0.8027 - val_loss: 0.8886 - val_acc: 0.8070\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8659 - acc: 0.8023 - val_loss: 0.9085 - val_acc: 0.7970\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8672 - acc: 0.8031 - val_loss: 0.8769 - val_acc: 0.8010\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8652 - acc: 0.8033 - val_loss: 0.8759 - val_acc: 0.7970\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8649 - acc: 0.8024 - val_loss: 0.8907 - val_acc: 0.8090\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.8033 - val_loss: 0.8782 - val_acc: 0.8100\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8647 - acc: 0.8015 - val_loss: 0.8902 - val_acc: 0.8070\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8646 - acc: 0.8024 - val_loss: 0.8900 - val_acc: 0.8070\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8656 - acc: 0.8016 - val_loss: 0.8824 - val_acc: 0.7940\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8632 - acc: 0.8027 - val_loss: 0.8766 - val_acc: 0.8000\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8633 - acc: 0.8023 - val_loss: 0.8864 - val_acc: 0.8080\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8637 - acc: 0.7996 - val_loss: 0.8763 - val_acc: 0.7940\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8653 - acc: 0.8025 - val_loss: 0.8916 - val_acc: 0.8070\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8623 - acc: 0.8029 - val_loss: 0.8786 - val_acc: 0.8020\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8634 - acc: 0.8049 - val_loss: 0.8982 - val_acc: 0.7990\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8631 - acc: 0.8013 - val_loss: 0.8786 - val_acc: 0.8100\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8613 - acc: 0.8037 - val_loss: 0.8759 - val_acc: 0.8010\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8620 - acc: 0.8015 - val_loss: 0.8888 - val_acc: 0.8060\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8635 - acc: 0.8012 - val_loss: 0.8885 - val_acc: 0.8090\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8615 - acc: 0.8047 - val_loss: 0.8893 - val_acc: 0.8060\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8615 - acc: 0.8000 - val_loss: 0.8828 - val_acc: 0.7930\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8624 - acc: 0.8019 - val_loss: 0.8817 - val_acc: 0.8070\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8613 - acc: 0.8047 - val_loss: 0.8806 - val_acc: 0.8110\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8601 - acc: 0.8028 - val_loss: 0.8752 - val_acc: 0.7990\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8616 - acc: 0.8013 - val_loss: 0.9000 - val_acc: 0.7970\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8612 - acc: 0.8019 - val_loss: 0.8910 - val_acc: 0.7990\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8612 - acc: 0.8017 - val_loss: 0.8725 - val_acc: 0.8000\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8604 - acc: 0.8033 - val_loss: 0.8741 - val_acc: 0.8100\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8605 - acc: 0.8045 - val_loss: 0.8870 - val_acc: 0.8040\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8610 - acc: 0.8040 - val_loss: 0.8932 - val_acc: 0.8080\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8583 - acc: 0.8053 - val_loss: 0.8826 - val_acc: 0.8100\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8590 - acc: 0.8048 - val_loss: 0.8792 - val_acc: 0.8030\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8581 - acc: 0.8044 - val_loss: 0.8854 - val_acc: 0.8050\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.8016 - val_loss: 0.8735 - val_acc: 0.8090\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8579 - acc: 0.8051 - val_loss: 0.8814 - val_acc: 0.8070\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8600 - acc: 0.8063 - val_loss: 0.8794 - val_acc: 0.8080\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8581 - acc: 0.8023 - val_loss: 0.8850 - val_acc: 0.8090\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.8057 - val_loss: 0.8842 - val_acc: 0.8140\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8571 - acc: 0.8051 - val_loss: 0.8764 - val_acc: 0.8040\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8579 - acc: 0.8023 - val_loss: 0.8794 - val_acc: 0.8000\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8591 - acc: 0.8036 - val_loss: 0.8821 - val_acc: 0.8070\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.8041 - val_loss: 0.8809 - val_acc: 0.8000\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8575 - acc: 0.8041 - val_loss: 0.8782 - val_acc: 0.8070\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8578 - acc: 0.8052 - val_loss: 0.8708 - val_acc: 0.7990\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8552 - acc: 0.8056 - val_loss: 0.8752 - val_acc: 0.8120\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8580 - acc: 0.8041 - val_loss: 0.8745 - val_acc: 0.8100\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8579 - acc: 0.8037 - val_loss: 0.8785 - val_acc: 0.8030\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8561 - acc: 0.8077 - val_loss: 0.8755 - val_acc: 0.8030\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8560 - acc: 0.8060 - val_loss: 0.8814 - val_acc: 0.8020\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8552 - acc: 0.8061 - val_loss: 0.8753 - val_acc: 0.7990\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8558 - acc: 0.8031 - val_loss: 0.8926 - val_acc: 0.8160\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8553 - acc: 0.8056 - val_loss: 0.8754 - val_acc: 0.8050\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8543 - acc: 0.8052 - val_loss: 0.9229 - val_acc: 0.7840\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8567 - acc: 0.8043 - val_loss: 0.8787 - val_acc: 0.8070\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8540 - acc: 0.8075 - val_loss: 0.8840 - val_acc: 0.8110\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8554 - acc: 0.8067 - val_loss: 0.8691 - val_acc: 0.8070\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8541 - acc: 0.8068 - val_loss: 0.8731 - val_acc: 0.8090\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8526 - acc: 0.8067 - val_loss: 0.8802 - val_acc: 0.8080\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8543 - acc: 0.8036 - val_loss: 0.8799 - val_acc: 0.8070\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8530 - acc: 0.8035 - val_loss: 0.8686 - val_acc: 0.8070\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8542 - acc: 0.8047 - val_loss: 0.8719 - val_acc: 0.7970\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8530 - acc: 0.8044 - val_loss: 0.8715 - val_acc: 0.8010\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8519 - acc: 0.8060 - val_loss: 0.8708 - val_acc: 0.8110\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8509 - acc: 0.8053 - val_loss: 0.8690 - val_acc: 0.8060\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8519 - acc: 0.8069 - val_loss: 0.8838 - val_acc: 0.7990\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.8085 - val_loss: 0.8682 - val_acc: 0.8100\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8520 - acc: 0.8069 - val_loss: 0.8855 - val_acc: 0.8150\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8507 - acc: 0.8073 - val_loss: 0.8826 - val_acc: 0.8140\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8518 - acc: 0.8045 - val_loss: 0.8690 - val_acc: 0.8030\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8523 - acc: 0.8059 - val_loss: 0.8697 - val_acc: 0.8070\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8510 - acc: 0.8072 - val_loss: 0.8781 - val_acc: 0.8110\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8524 - acc: 0.8057 - val_loss: 0.8721 - val_acc: 0.8160\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8516 - acc: 0.8075 - val_loss: 0.8692 - val_acc: 0.8060\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8512 - acc: 0.8044 - val_loss: 0.8994 - val_acc: 0.8010\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8516 - acc: 0.8064 - val_loss: 0.8684 - val_acc: 0.8080\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8503 - acc: 0.8045 - val_loss: 0.8683 - val_acc: 0.8040\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8522 - acc: 0.8071 - val_loss: 0.8701 - val_acc: 0.7960\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.8068 - val_loss: 0.8778 - val_acc: 0.8060\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8500 - acc: 0.8049 - val_loss: 0.8727 - val_acc: 0.8140\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8496 - acc: 0.8077 - val_loss: 0.8716 - val_acc: 0.8010\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8495 - acc: 0.8079 - val_loss: 0.8656 - val_acc: 0.7980\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8498 - acc: 0.8061 - val_loss: 0.8737 - val_acc: 0.8050\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8494 - acc: 0.8097 - val_loss: 0.8775 - val_acc: 0.8100\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8485 - acc: 0.8053 - val_loss: 0.8789 - val_acc: 0.7980\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.8064 - val_loss: 0.8743 - val_acc: 0.7960\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8492 - acc: 0.8083 - val_loss: 0.8697 - val_acc: 0.8140\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8474 - acc: 0.8096 - val_loss: 0.8937 - val_acc: 0.7980\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8461 - acc: 0.8085 - val_loss: 0.9062 - val_acc: 0.7970\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8482 - acc: 0.8080 - val_loss: 0.8727 - val_acc: 0.8120\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8480 - acc: 0.8053 - val_loss: 0.8752 - val_acc: 0.8140\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8452 - acc: 0.8091 - val_loss: 0.8955 - val_acc: 0.7970\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8482 - acc: 0.8068 - val_loss: 0.8765 - val_acc: 0.8070\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8468 - acc: 0.8095 - val_loss: 0.8707 - val_acc: 0.8010\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8474 - acc: 0.8041 - val_loss: 0.8805 - val_acc: 0.8080\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8458 - acc: 0.8099 - val_loss: 0.8784 - val_acc: 0.8100\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8465 - acc: 0.8063 - val_loss: 0.8678 - val_acc: 0.8140\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8465 - acc: 0.8057 - val_loss: 0.8917 - val_acc: 0.7970\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8473 - acc: 0.8072 - val_loss: 0.8677 - val_acc: 0.7980\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8476 - acc: 0.8079 - val_loss: 0.8648 - val_acc: 0.8010\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8440 - acc: 0.8096 - val_loss: 0.9240 - val_acc: 0.7820\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8471 - acc: 0.8055 - val_loss: 0.8702 - val_acc: 0.8050\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8454 - acc: 0.8073 - val_loss: 0.8785 - val_acc: 0.8050\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8464 - acc: 0.8083 - val_loss: 0.8672 - val_acc: 0.8120\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8440 - acc: 0.8081 - val_loss: 0.8726 - val_acc: 0.8030\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8449 - acc: 0.8092 - val_loss: 0.8657 - val_acc: 0.8070\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8439 - acc: 0.8069 - val_loss: 0.8657 - val_acc: 0.8050\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8427 - acc: 0.8108 - val_loss: 0.8651 - val_acc: 0.8010\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8438 - acc: 0.8101 - val_loss: 0.8693 - val_acc: 0.8020\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8444 - acc: 0.8075 - val_loss: 0.8750 - val_acc: 0.7950\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8451 - acc: 0.8108 - val_loss: 0.8662 - val_acc: 0.8050\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8421 - acc: 0.8093 - val_loss: 0.8695 - val_acc: 0.8050\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8428 - acc: 0.8096 - val_loss: 0.8642 - val_acc: 0.8000\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8428 - acc: 0.8104 - val_loss: 0.8632 - val_acc: 0.8090\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.8116 - val_loss: 0.8946 - val_acc: 0.7890\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8428 - acc: 0.8089 - val_loss: 0.8647 - val_acc: 0.8150\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8407 - acc: 0.8100 - val_loss: 0.8632 - val_acc: 0.8010\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8419 - acc: 0.8088 - val_loss: 0.8630 - val_acc: 0.8080\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8405 - acc: 0.8124 - val_loss: 0.8675 - val_acc: 0.8100\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8446 - acc: 0.8093 - val_loss: 0.8646 - val_acc: 0.8130\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.8096 - val_loss: 0.8710 - val_acc: 0.7980\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8415 - acc: 0.8092 - val_loss: 0.8693 - val_acc: 0.8000\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8422 - acc: 0.8053 - val_loss: 0.8924 - val_acc: 0.8040\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8427 - acc: 0.8089 - val_loss: 0.8671 - val_acc: 0.8060\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8411 - acc: 0.8091 - val_loss: 0.8679 - val_acc: 0.8060\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8401 - acc: 0.8112 - val_loss: 0.8682 - val_acc: 0.8000\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8397 - acc: 0.8115 - val_loss: 0.8683 - val_acc: 0.8020\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8403 - acc: 0.8120 - val_loss: 0.8717 - val_acc: 0.8130\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8404 - acc: 0.8120 - val_loss: 0.8849 - val_acc: 0.8080\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8399 - acc: 0.8125 - val_loss: 0.8775 - val_acc: 0.7960\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8399 - acc: 0.8132 - val_loss: 0.8966 - val_acc: 0.7970\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8425 - acc: 0.8069 - val_loss: 0.8712 - val_acc: 0.7990\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8407 - acc: 0.8112 - val_loss: 0.8663 - val_acc: 0.8020\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8396 - acc: 0.8099 - val_loss: 0.8761 - val_acc: 0.8080\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8391 - acc: 0.8113 - val_loss: 0.8894 - val_acc: 0.7980\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8390 - acc: 0.8117 - val_loss: 0.8799 - val_acc: 0.8080\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8388 - acc: 0.8123 - val_loss: 0.8629 - val_acc: 0.7980\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8394 - acc: 0.8137 - val_loss: 0.8652 - val_acc: 0.8060\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8379 - acc: 0.8111 - val_loss: 0.8673 - val_acc: 0.8000\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8404 - acc: 0.8137 - val_loss: 0.8658 - val_acc: 0.8030\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8375 - acc: 0.8136 - val_loss: 0.8709 - val_acc: 0.8090\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8372 - acc: 0.8133 - val_loss: 0.8775 - val_acc: 0.8100\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8367 - acc: 0.8109 - val_loss: 0.8800 - val_acc: 0.8100\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8391 - acc: 0.8103 - val_loss: 0.8856 - val_acc: 0.7990\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8386 - acc: 0.8113 - val_loss: 0.8957 - val_acc: 0.7870\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8374 - acc: 0.8141 - val_loss: 0.8678 - val_acc: 0.8000\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8373 - acc: 0.8119 - val_loss: 0.8667 - val_acc: 0.8000\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.8131 - val_loss: 0.8588 - val_acc: 0.8040\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8342 - acc: 0.8147 - val_loss: 0.8642 - val_acc: 0.8160\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8372 - acc: 0.8088 - val_loss: 0.8832 - val_acc: 0.7990\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8367 - acc: 0.8128 - val_loss: 0.8782 - val_acc: 0.8080\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8364 - acc: 0.8128 - val_loss: 0.8668 - val_acc: 0.8100\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8355 - acc: 0.8113 - val_loss: 0.8676 - val_acc: 0.8150\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8348 - acc: 0.8107 - val_loss: 0.8733 - val_acc: 0.8090\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8344 - acc: 0.8127 - val_loss: 0.8695 - val_acc: 0.7950\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8360 - acc: 0.8121 - val_loss: 0.8681 - val_acc: 0.8110\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8352 - acc: 0.8136 - val_loss: 0.8572 - val_acc: 0.8020\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8337 - acc: 0.8147 - val_loss: 0.8665 - val_acc: 0.8100\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8333 - acc: 0.8135 - val_loss: 0.8781 - val_acc: 0.8060\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8360 - acc: 0.8108 - val_loss: 0.8742 - val_acc: 0.8100\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8361 - acc: 0.8127 - val_loss: 0.8832 - val_acc: 0.8110\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8344 - acc: 0.8139 - val_loss: 0.8732 - val_acc: 0.8120\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8321 - acc: 0.8131 - val_loss: 0.8642 - val_acc: 0.8160\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8369 - acc: 0.8135 - val_loss: 0.8655 - val_acc: 0.8030\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8320 - acc: 0.8119 - val_loss: 0.8670 - val_acc: 0.7970\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8332 - acc: 0.8127 - val_loss: 0.8605 - val_acc: 0.8010\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8351 - acc: 0.8115 - val_loss: 0.8771 - val_acc: 0.8030\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8356 - acc: 0.8135 - val_loss: 0.8762 - val_acc: 0.8100\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8321 - acc: 0.8127 - val_loss: 0.8606 - val_acc: 0.8050\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8318 - acc: 0.8147 - val_loss: 0.8658 - val_acc: 0.8180\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8327 - acc: 0.8153 - val_loss: 0.8780 - val_acc: 0.8010\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8326 - acc: 0.8141 - val_loss: 0.8586 - val_acc: 0.8010\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8322 - acc: 0.8151 - val_loss: 0.8706 - val_acc: 0.8100\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8316 - acc: 0.8141 - val_loss: 0.8676 - val_acc: 0.8150\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8326 - acc: 0.8148 - val_loss: 0.8625 - val_acc: 0.7990\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8323 - acc: 0.8143 - val_loss: 0.8683 - val_acc: 0.8090\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8317 - acc: 0.8147 - val_loss: 0.8828 - val_acc: 0.8030\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8327 - acc: 0.8144 - val_loss: 0.8573 - val_acc: 0.8040\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8291 - acc: 0.8152 - val_loss: 0.8557 - val_acc: 0.8030\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8318 - acc: 0.8127 - val_loss: 0.8681 - val_acc: 0.8120\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8297 - acc: 0.8128 - val_loss: 0.8615 - val_acc: 0.8010\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8320 - acc: 0.8155 - val_loss: 0.8587 - val_acc: 0.7990\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8311 - acc: 0.8160 - val_loss: 0.9049 - val_acc: 0.7860\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8315 - acc: 0.8157 - val_loss: 0.8787 - val_acc: 0.8040\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8284 - acc: 0.8173 - val_loss: 0.8671 - val_acc: 0.8010\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8296 - acc: 0.8152 - val_loss: 0.8705 - val_acc: 0.8060\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8327 - acc: 0.8135 - val_loss: 0.8581 - val_acc: 0.7960\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8290 - acc: 0.8168 - val_loss: 0.9130 - val_acc: 0.7890\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.8140 - val_loss: 0.8669 - val_acc: 0.8040\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8295 - acc: 0.8159 - val_loss: 0.8564 - val_acc: 0.8110\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8288 - acc: 0.8167 - val_loss: 0.8603 - val_acc: 0.8080\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8304 - acc: 0.8135 - val_loss: 0.8575 - val_acc: 0.8110\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8294 - acc: 0.8168 - val_loss: 0.8565 - val_acc: 0.8020\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8279 - acc: 0.8145 - val_loss: 0.8601 - val_acc: 0.8130\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8279 - acc: 0.8140 - val_loss: 0.8810 - val_acc: 0.8040\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8304 - acc: 0.8152 - val_loss: 0.8638 - val_acc: 0.8030\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.8144 - val_loss: 0.8742 - val_acc: 0.7910\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8275 - acc: 0.8191 - val_loss: 0.8631 - val_acc: 0.7990\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8306 - acc: 0.8145 - val_loss: 0.8699 - val_acc: 0.7950\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8310 - acc: 0.8192 - val_loss: 0.8725 - val_acc: 0.8060\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8310 - acc: 0.8148 - val_loss: 0.8780 - val_acc: 0.7980\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8284 - acc: 0.8171 - val_loss: 0.8686 - val_acc: 0.8070\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8286 - acc: 0.8156 - val_loss: 0.8600 - val_acc: 0.8010\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8253 - acc: 0.8177 - val_loss: 0.8588 - val_acc: 0.8050\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8261 - acc: 0.8156 - val_loss: 0.8654 - val_acc: 0.8100\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8276 - acc: 0.8163 - val_loss: 0.8616 - val_acc: 0.8130\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8251 - acc: 0.8181 - val_loss: 0.8575 - val_acc: 0.8070\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8286 - acc: 0.8116 - val_loss: 0.8596 - val_acc: 0.8160\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8272 - acc: 0.8176 - val_loss: 0.8606 - val_acc: 0.8110\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8266 - acc: 0.8165 - val_loss: 0.8536 - val_acc: 0.8070\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8251 - acc: 0.8177 - val_loss: 0.8705 - val_acc: 0.8090\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8263 - acc: 0.8176 - val_loss: 0.8686 - val_acc: 0.8090\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8255 - acc: 0.8185 - val_loss: 0.8719 - val_acc: 0.8060\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8263 - acc: 0.8179 - val_loss: 0.8609 - val_acc: 0.8060\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8234 - acc: 0.8188 - val_loss: 0.8820 - val_acc: 0.7990\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8260 - acc: 0.8156 - val_loss: 0.8964 - val_acc: 0.7900\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8286 - acc: 0.8173 - val_loss: 0.8645 - val_acc: 0.8090\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8247 - acc: 0.8141 - val_loss: 0.8586 - val_acc: 0.8050\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8230 - acc: 0.8195 - val_loss: 0.8604 - val_acc: 0.8040\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8246 - acc: 0.8172 - val_loss: 0.8720 - val_acc: 0.8000\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8274 - acc: 0.8195 - val_loss: 0.8700 - val_acc: 0.8090\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8249 - acc: 0.8164 - val_loss: 0.8744 - val_acc: 0.8070\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8256 - acc: 0.8173 - val_loss: 0.8558 - val_acc: 0.8070\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8243 - acc: 0.8176 - val_loss: 0.8674 - val_acc: 0.8120\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8231 - acc: 0.8199 - val_loss: 0.8914 - val_acc: 0.7960\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8247 - acc: 0.8152 - val_loss: 0.8759 - val_acc: 0.8080\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8255 - acc: 0.8161 - val_loss: 0.8605 - val_acc: 0.8080\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8251 - acc: 0.8187 - val_loss: 0.8558 - val_acc: 0.8060\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8217 - acc: 0.8191 - val_loss: 0.8904 - val_acc: 0.7980\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8220 - acc: 0.8183 - val_loss: 0.8641 - val_acc: 0.8030\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8232 - acc: 0.8180 - val_loss: 0.8683 - val_acc: 0.7960\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8242 - acc: 0.8167 - val_loss: 0.8830 - val_acc: 0.8030\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8215 - acc: 0.8184 - val_loss: 0.8688 - val_acc: 0.8040\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8228 - acc: 0.8173 - val_loss: 0.8708 - val_acc: 0.8100\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8216 - acc: 0.8192 - val_loss: 0.9003 - val_acc: 0.7980\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8255 - acc: 0.8163 - val_loss: 0.8648 - val_acc: 0.8140\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8225 - acc: 0.8189 - val_loss: 0.8846 - val_acc: 0.8040\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8229 - acc: 0.8163 - val_loss: 0.8844 - val_acc: 0.7930\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8230 - acc: 0.8199 - val_loss: 0.8671 - val_acc: 0.8140\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8231 - acc: 0.8176 - val_loss: 0.8612 - val_acc: 0.7960\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8210 - acc: 0.8207 - val_loss: 0.8554 - val_acc: 0.8120\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8198 - acc: 0.8185 - val_loss: 0.8594 - val_acc: 0.8140\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8191 - acc: 0.8184 - val_loss: 0.8780 - val_acc: 0.8030\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8204 - acc: 0.8175 - val_loss: 0.8610 - val_acc: 0.8040\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8213 - acc: 0.8223 - val_loss: 0.8789 - val_acc: 0.7990\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8200 - acc: 0.8209 - val_loss: 0.8698 - val_acc: 0.8040\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8200 - acc: 0.8185 - val_loss: 0.8667 - val_acc: 0.8120\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8199 - acc: 0.8184 - val_loss: 0.9272 - val_acc: 0.7900\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8270 - acc: 0.8140 - val_loss: 0.8785 - val_acc: 0.8090\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8208 - acc: 0.8207 - val_loss: 0.8504 - val_acc: 0.8000\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8182 - acc: 0.8213 - val_loss: 0.8514 - val_acc: 0.8110\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8190 - acc: 0.8177 - val_loss: 0.8662 - val_acc: 0.8010\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8189 - acc: 0.8199 - val_loss: 0.8744 - val_acc: 0.8060\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8174 - acc: 0.8213 - val_loss: 0.8788 - val_acc: 0.8010\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8190 - acc: 0.8201 - val_loss: 0.8568 - val_acc: 0.8170\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8200 - acc: 0.8197 - val_loss: 0.8584 - val_acc: 0.8050\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8187 - acc: 0.8228 - val_loss: 0.8546 - val_acc: 0.8120\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8195 - acc: 0.8196 - val_loss: 0.8808 - val_acc: 0.7980\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8182 - acc: 0.8211 - val_loss: 0.8590 - val_acc: 0.8000\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8182 - acc: 0.8196 - val_loss: 0.8562 - val_acc: 0.8030\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8167 - acc: 0.8219 - val_loss: 0.8539 - val_acc: 0.8170\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8161 - acc: 0.8207 - val_loss: 0.8599 - val_acc: 0.8110\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8184 - acc: 0.8219 - val_loss: 0.8710 - val_acc: 0.7990\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8178 - acc: 0.8224 - val_loss: 0.8673 - val_acc: 0.8090\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8174 - acc: 0.8196 - val_loss: 0.8696 - val_acc: 0.8010\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8165 - acc: 0.8233 - val_loss: 0.8647 - val_acc: 0.8130\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8167 - acc: 0.8197 - val_loss: 0.8654 - val_acc: 0.8100\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8163 - acc: 0.8229 - val_loss: 0.8732 - val_acc: 0.7980\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8178 - acc: 0.8193 - val_loss: 0.8631 - val_acc: 0.8040\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8191 - acc: 0.8199 - val_loss: 0.8509 - val_acc: 0.8060\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8159 - acc: 0.8224 - val_loss: 0.8594 - val_acc: 0.8050\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8168 - acc: 0.8224 - val_loss: 0.8582 - val_acc: 0.8070\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8170 - acc: 0.8216 - val_loss: 0.8713 - val_acc: 0.8040\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8170 - acc: 0.8225 - val_loss: 0.8584 - val_acc: 0.8150\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8163 - acc: 0.8211 - val_loss: 0.8554 - val_acc: 0.8140\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8157 - acc: 0.8231 - val_loss: 0.8545 - val_acc: 0.8030\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8143 - acc: 0.8247 - val_loss: 0.8610 - val_acc: 0.8000\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8149 - acc: 0.8217 - val_loss: 0.8685 - val_acc: 0.8070\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8161 - acc: 0.8211 - val_loss: 0.8737 - val_acc: 0.8020\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8156 - acc: 0.8229 - val_loss: 0.8579 - val_acc: 0.8170\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8142 - acc: 0.8239 - val_loss: 0.8577 - val_acc: 0.8160\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8149 - acc: 0.8224 - val_loss: 0.8683 - val_acc: 0.8000\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8168 - acc: 0.8203 - val_loss: 0.8549 - val_acc: 0.8110\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8140 - acc: 0.8207 - val_loss: 0.8645 - val_acc: 0.8070\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8158 - acc: 0.8204 - val_loss: 0.8731 - val_acc: 0.8040\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8158 - acc: 0.8224 - val_loss: 0.8627 - val_acc: 0.8030\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8133 - acc: 0.8231 - val_loss: 0.8954 - val_acc: 0.7950\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8156 - acc: 0.8216 - val_loss: 0.8671 - val_acc: 0.8070\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8123 - acc: 0.8237 - val_loss: 0.8593 - val_acc: 0.8140\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8137 - acc: 0.8236 - val_loss: 0.8625 - val_acc: 0.8060\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8148 - acc: 0.8229 - val_loss: 0.8520 - val_acc: 0.8080\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8146 - acc: 0.8248 - val_loss: 0.8576 - val_acc: 0.7990\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8150 - acc: 0.8228 - val_loss: 0.8618 - val_acc: 0.8080\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8127 - acc: 0.8220 - val_loss: 0.8610 - val_acc: 0.8080\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8121 - acc: 0.8191 - val_loss: 0.8667 - val_acc: 0.7920\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8147 - acc: 0.8212 - val_loss: 0.8532 - val_acc: 0.8090\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8120 - acc: 0.8227 - val_loss: 0.8539 - val_acc: 0.8120\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8133 - acc: 0.8209 - val_loss: 0.8544 - val_acc: 0.8030\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8132 - acc: 0.8221 - val_loss: 0.8560 - val_acc: 0.8170\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8131 - acc: 0.8205 - val_loss: 0.8551 - val_acc: 0.8020\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8122 - acc: 0.8229 - val_loss: 0.8583 - val_acc: 0.7980\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8129 - acc: 0.8211 - val_loss: 0.8874 - val_acc: 0.8010\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8148 - acc: 0.8225 - val_loss: 0.8643 - val_acc: 0.8010\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8120 - acc: 0.8248 - val_loss: 0.8518 - val_acc: 0.8070\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8149 - acc: 0.8204 - val_loss: 0.8854 - val_acc: 0.8020\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8128 - acc: 0.8216 - val_loss: 0.8657 - val_acc: 0.7980\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8171 - acc: 0.8207 - val_loss: 0.8662 - val_acc: 0.8120\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8118 - acc: 0.8227 - val_loss: 0.8523 - val_acc: 0.8030\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8118 - acc: 0.8255 - val_loss: 0.8776 - val_acc: 0.8020\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8144 - acc: 0.8203 - val_loss: 0.8808 - val_acc: 0.8060\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8123 - acc: 0.8233 - val_loss: 0.8572 - val_acc: 0.8060\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8092 - acc: 0.8224 - val_loss: 0.8960 - val_acc: 0.7930\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8138 - acc: 0.8225 - val_loss: 0.8583 - val_acc: 0.8070\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8103 - acc: 0.8243 - val_loss: 0.8992 - val_acc: 0.7950\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8104 - acc: 0.8221 - val_loss: 0.8722 - val_acc: 0.8080\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8111 - acc: 0.8259 - val_loss: 0.8473 - val_acc: 0.8080\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8087 - acc: 0.8256 - val_loss: 0.8914 - val_acc: 0.7930\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8122 - acc: 0.8231 - val_loss: 0.8543 - val_acc: 0.8060\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8115 - acc: 0.8203 - val_loss: 0.8647 - val_acc: 0.7920\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8118 - acc: 0.8215 - val_loss: 0.8639 - val_acc: 0.8090\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8113 - acc: 0.8228 - val_loss: 0.8633 - val_acc: 0.8120\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8084 - acc: 0.8248 - val_loss: 0.8519 - val_acc: 0.8110\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8087 - acc: 0.8240 - val_loss: 0.8832 - val_acc: 0.7870\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8119 - acc: 0.8224 - val_loss: 0.8617 - val_acc: 0.8080\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8103 - acc: 0.8235 - val_loss: 0.8905 - val_acc: 0.7930\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8096 - acc: 0.8251 - val_loss: 0.8619 - val_acc: 0.8070\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8089 - acc: 0.8271 - val_loss: 0.8589 - val_acc: 0.8010\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8079 - acc: 0.8249 - val_loss: 0.8524 - val_acc: 0.8030\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8075 - acc: 0.8265 - val_loss: 0.8530 - val_acc: 0.8040\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8070 - acc: 0.8265 - val_loss: 0.8717 - val_acc: 0.7990\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8073 - acc: 0.8231 - val_loss: 0.8558 - val_acc: 0.8140\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8091 - acc: 0.8235 - val_loss: 0.8837 - val_acc: 0.7950\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8076 - acc: 0.8260 - val_loss: 0.8904 - val_acc: 0.7990\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8094 - acc: 0.8227 - val_loss: 0.8796 - val_acc: 0.8080\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8098 - acc: 0.8273 - val_loss: 0.8737 - val_acc: 0.8050\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8086 - acc: 0.8283 - val_loss: 0.8619 - val_acc: 0.8020\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8081 - acc: 0.8260 - val_loss: 0.8650 - val_acc: 0.8060\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8104 - acc: 0.8241 - val_loss: 0.8597 - val_acc: 0.8020\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8069 - acc: 0.8263 - val_loss: 0.8718 - val_acc: 0.7990\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8068 - acc: 0.8251 - val_loss: 0.8587 - val_acc: 0.7990\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8076 - acc: 0.8251 - val_loss: 0.8468 - val_acc: 0.8030\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8066 - acc: 0.8256 - val_loss: 0.8563 - val_acc: 0.8010\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8125 - acc: 0.8221 - val_loss: 0.8571 - val_acc: 0.8150\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8066 - acc: 0.8257 - val_loss: 0.8647 - val_acc: 0.8120\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8058 - acc: 0.8284 - val_loss: 0.8474 - val_acc: 0.8010\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8075 - acc: 0.8265 - val_loss: 0.8504 - val_acc: 0.8060\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8051 - acc: 0.8217 - val_loss: 0.8837 - val_acc: 0.8020\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8071 - acc: 0.8248 - val_loss: 0.8557 - val_acc: 0.8080\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8069 - acc: 0.8256 - val_loss: 0.8454 - val_acc: 0.8100\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8050 - acc: 0.8275 - val_loss: 0.8710 - val_acc: 0.7960\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8036 - acc: 0.8251 - val_loss: 0.8471 - val_acc: 0.8070\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8046 - acc: 0.8264 - val_loss: 0.8558 - val_acc: 0.8050\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8053 - acc: 0.8255 - val_loss: 0.8564 - val_acc: 0.8080\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step\n",
      "1500/1500 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7996308924992879, 0.8276000000317891]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.913732646783193, 0.7820000001589458]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.9827 - acc: 0.1412 - val_loss: 1.9428 - val_acc: 0.1340\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9611 - acc: 0.1455 - val_loss: 1.9335 - val_acc: 0.1600\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.9425 - acc: 0.1647 - val_loss: 1.9273 - val_acc: 0.1770\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9322 - acc: 0.1667 - val_loss: 1.9213 - val_acc: 0.1910\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9272 - acc: 0.1785 - val_loss: 1.9148 - val_acc: 0.1950\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9220 - acc: 0.1781 - val_loss: 1.9076 - val_acc: 0.1990\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9115 - acc: 0.1868 - val_loss: 1.8990 - val_acc: 0.2200\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9045 - acc: 0.2015 - val_loss: 1.8886 - val_acc: 0.2370\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8964 - acc: 0.2028 - val_loss: 1.8769 - val_acc: 0.2490\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8863 - acc: 0.2179 - val_loss: 1.8631 - val_acc: 0.2730\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.8695 - acc: 0.2343 - val_loss: 1.8457 - val_acc: 0.2930\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8601 - acc: 0.2303 - val_loss: 1.8278 - val_acc: 0.3130\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8485 - acc: 0.2327 - val_loss: 1.8105 - val_acc: 0.3210\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8320 - acc: 0.2480 - val_loss: 1.7898 - val_acc: 0.3400\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8207 - acc: 0.2524 - val_loss: 1.7691 - val_acc: 0.3400\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8052 - acc: 0.2588 - val_loss: 1.7462 - val_acc: 0.3530\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7883 - acc: 0.2664 - val_loss: 1.7231 - val_acc: 0.3680\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7751 - acc: 0.2771 - val_loss: 1.7011 - val_acc: 0.3830\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7549 - acc: 0.2915 - val_loss: 1.6745 - val_acc: 0.3880\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7439 - acc: 0.2944 - val_loss: 1.6523 - val_acc: 0.3980\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7197 - acc: 0.3056 - val_loss: 1.6265 - val_acc: 0.4040\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7001 - acc: 0.3092 - val_loss: 1.6020 - val_acc: 0.4200\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6798 - acc: 0.3221 - val_loss: 1.5771 - val_acc: 0.4330\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6616 - acc: 0.3317 - val_loss: 1.5507 - val_acc: 0.4500\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6488 - acc: 0.3383 - val_loss: 1.5256 - val_acc: 0.4630\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6343 - acc: 0.3451 - val_loss: 1.5020 - val_acc: 0.4630\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6160 - acc: 0.3507 - val_loss: 1.4764 - val_acc: 0.4770\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5960 - acc: 0.3672 - val_loss: 1.4521 - val_acc: 0.5000\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5749 - acc: 0.3733 - val_loss: 1.4308 - val_acc: 0.5020\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5726 - acc: 0.3727 - val_loss: 1.4104 - val_acc: 0.5170\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5518 - acc: 0.3805 - val_loss: 1.3912 - val_acc: 0.5170\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5431 - acc: 0.3799 - val_loss: 1.3706 - val_acc: 0.5200\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5361 - acc: 0.3843 - val_loss: 1.3537 - val_acc: 0.5450\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5128 - acc: 0.3941 - val_loss: 1.3327 - val_acc: 0.5620\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4976 - acc: 0.4044 - val_loss: 1.3125 - val_acc: 0.5700\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4787 - acc: 0.4160 - val_loss: 1.2941 - val_acc: 0.5820\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4778 - acc: 0.4149 - val_loss: 1.2797 - val_acc: 0.5910\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4463 - acc: 0.4335 - val_loss: 1.2616 - val_acc: 0.5930\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4418 - acc: 0.4324 - val_loss: 1.2451 - val_acc: 0.6140\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4374 - acc: 0.4404 - val_loss: 1.2270 - val_acc: 0.6180\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4188 - acc: 0.4405 - val_loss: 1.2123 - val_acc: 0.6140\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4106 - acc: 0.4471 - val_loss: 1.1983 - val_acc: 0.6220\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4024 - acc: 0.4572 - val_loss: 1.1827 - val_acc: 0.6280\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3813 - acc: 0.4529 - val_loss: 1.1679 - val_acc: 0.6420\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3743 - acc: 0.4656 - val_loss: 1.1525 - val_acc: 0.6350\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3624 - acc: 0.4716 - val_loss: 1.1392 - val_acc: 0.6550\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3532 - acc: 0.4800 - val_loss: 1.1274 - val_acc: 0.6420\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3525 - acc: 0.4712 - val_loss: 1.1171 - val_acc: 0.6520\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3284 - acc: 0.4903 - val_loss: 1.1029 - val_acc: 0.6590\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3216 - acc: 0.4859 - val_loss: 1.0910 - val_acc: 0.6660\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3213 - acc: 0.4871 - val_loss: 1.0785 - val_acc: 0.6680\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3047 - acc: 0.4925 - val_loss: 1.0709 - val_acc: 0.6710\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2917 - acc: 0.5027 - val_loss: 1.0555 - val_acc: 0.6710\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2830 - acc: 0.5037 - val_loss: 1.0423 - val_acc: 0.6840\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2786 - acc: 0.5025 - val_loss: 1.0321 - val_acc: 0.6790\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2688 - acc: 0.5125 - val_loss: 1.0234 - val_acc: 0.6840\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2453 - acc: 0.5216 - val_loss: 1.0128 - val_acc: 0.6880\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2418 - acc: 0.5263 - val_loss: 1.0020 - val_acc: 0.6900\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2522 - acc: 0.5224 - val_loss: 0.9944 - val_acc: 0.6860\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2372 - acc: 0.5275 - val_loss: 0.9858 - val_acc: 0.6930\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2283 - acc: 0.5261 - val_loss: 0.9744 - val_acc: 0.7050\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2214 - acc: 0.5320 - val_loss: 0.9668 - val_acc: 0.6990\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2137 - acc: 0.5316 - val_loss: 0.9595 - val_acc: 0.6970\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2109 - acc: 0.5389 - val_loss: 0.9544 - val_acc: 0.6970\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2019 - acc: 0.5357 - val_loss: 0.9451 - val_acc: 0.7010\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1990 - acc: 0.5477 - val_loss: 0.9380 - val_acc: 0.7080\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1801 - acc: 0.5476 - val_loss: 0.9308 - val_acc: 0.7110\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.1740 - acc: 0.5567 - val_loss: 0.9219 - val_acc: 0.7110\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1694 - acc: 0.5604 - val_loss: 0.9128 - val_acc: 0.7200\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1790 - acc: 0.5564 - val_loss: 0.9062 - val_acc: 0.7200\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1475 - acc: 0.5613 - val_loss: 0.8964 - val_acc: 0.7140\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1416 - acc: 0.5648 - val_loss: 0.8889 - val_acc: 0.7220\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1328 - acc: 0.5749 - val_loss: 0.8812 - val_acc: 0.7240\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1346 - acc: 0.5791 - val_loss: 0.8761 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1281 - acc: 0.5699 - val_loss: 0.8725 - val_acc: 0.7190\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1137 - acc: 0.5799 - val_loss: 0.8649 - val_acc: 0.7240\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1205 - acc: 0.5745 - val_loss: 0.8579 - val_acc: 0.7260\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1064 - acc: 0.5785 - val_loss: 0.8483 - val_acc: 0.7300\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1128 - acc: 0.5819 - val_loss: 0.8441 - val_acc: 0.7310\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0982 - acc: 0.5833 - val_loss: 0.8388 - val_acc: 0.7280\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0879 - acc: 0.5871 - val_loss: 0.8326 - val_acc: 0.7250\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0866 - acc: 0.5864 - val_loss: 0.8287 - val_acc: 0.7280\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0778 - acc: 0.5969 - val_loss: 0.8200 - val_acc: 0.7330\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0862 - acc: 0.5857 - val_loss: 0.8136 - val_acc: 0.7380\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0757 - acc: 0.5933 - val_loss: 0.8098 - val_acc: 0.7370\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0654 - acc: 0.6057 - val_loss: 0.8048 - val_acc: 0.7350\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0547 - acc: 0.6085 - val_loss: 0.7956 - val_acc: 0.7340\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0518 - acc: 0.6039 - val_loss: 0.7903 - val_acc: 0.7450\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0403 - acc: 0.6035 - val_loss: 0.7855 - val_acc: 0.7510\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0515 - acc: 0.5981 - val_loss: 0.7829 - val_acc: 0.7510\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0408 - acc: 0.6125 - val_loss: 0.7760 - val_acc: 0.7540\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0472 - acc: 0.6037 - val_loss: 0.7784 - val_acc: 0.7480\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0389 - acc: 0.6125 - val_loss: 0.7721 - val_acc: 0.7520\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0223 - acc: 0.6168 - val_loss: 0.7666 - val_acc: 0.7530\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0245 - acc: 0.6147 - val_loss: 0.7653 - val_acc: 0.7490\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0286 - acc: 0.6077 - val_loss: 0.7621 - val_acc: 0.7560\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0043 - acc: 0.6247 - val_loss: 0.7549 - val_acc: 0.7560\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0158 - acc: 0.6136 - val_loss: 0.7558 - val_acc: 0.7530\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9965 - acc: 0.6308 - val_loss: 0.7478 - val_acc: 0.7510\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9990 - acc: 0.6211 - val_loss: 0.7402 - val_acc: 0.7530\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9951 - acc: 0.6287 - val_loss: 0.7390 - val_acc: 0.7520\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9985 - acc: 0.6255 - val_loss: 0.7335 - val_acc: 0.7570\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9679 - acc: 0.6379 - val_loss: 0.7304 - val_acc: 0.7610\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9807 - acc: 0.6285 - val_loss: 0.7300 - val_acc: 0.7570\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9641 - acc: 0.6408 - val_loss: 0.7195 - val_acc: 0.7660\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9776 - acc: 0.6260 - val_loss: 0.7196 - val_acc: 0.7610\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9666 - acc: 0.6403 - val_loss: 0.7163 - val_acc: 0.7620\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9686 - acc: 0.6360 - val_loss: 0.7151 - val_acc: 0.7580\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9604 - acc: 0.6391 - val_loss: 0.7090 - val_acc: 0.7670\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9580 - acc: 0.6456 - val_loss: 0.7064 - val_acc: 0.7640\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9461 - acc: 0.6491 - val_loss: 0.7013 - val_acc: 0.7650\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9666 - acc: 0.6337 - val_loss: 0.7019 - val_acc: 0.7590\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9503 - acc: 0.6447 - val_loss: 0.6985 - val_acc: 0.7600\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9400 - acc: 0.6491 - val_loss: 0.6952 - val_acc: 0.7640\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9426 - acc: 0.6507 - val_loss: 0.6924 - val_acc: 0.7630\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9321 - acc: 0.6545 - val_loss: 0.6901 - val_acc: 0.7570\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9371 - acc: 0.6540 - val_loss: 0.6866 - val_acc: 0.7640\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9159 - acc: 0.6576 - val_loss: 0.6804 - val_acc: 0.7720\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9185 - acc: 0.6549 - val_loss: 0.6796 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9317 - acc: 0.6421 - val_loss: 0.6770 - val_acc: 0.7670\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9210 - acc: 0.6523 - val_loss: 0.6733 - val_acc: 0.7700\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9143 - acc: 0.6521 - val_loss: 0.6711 - val_acc: 0.7700\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9116 - acc: 0.6649 - val_loss: 0.6691 - val_acc: 0.7660\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9075 - acc: 0.6583 - val_loss: 0.6659 - val_acc: 0.7710\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9109 - acc: 0.6612 - val_loss: 0.6683 - val_acc: 0.7670\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9084 - acc: 0.6603 - val_loss: 0.6651 - val_acc: 0.7680\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9189 - acc: 0.6551 - val_loss: 0.6654 - val_acc: 0.7690\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8907 - acc: 0.6639 - val_loss: 0.6583 - val_acc: 0.7740\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9063 - acc: 0.6595 - val_loss: 0.6578 - val_acc: 0.7670\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9069 - acc: 0.6632 - val_loss: 0.6556 - val_acc: 0.7730\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8938 - acc: 0.6671 - val_loss: 0.6526 - val_acc: 0.7720\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8872 - acc: 0.6640 - val_loss: 0.6472 - val_acc: 0.7700\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8962 - acc: 0.6659 - val_loss: 0.6439 - val_acc: 0.7730\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8958 - acc: 0.6636 - val_loss: 0.6442 - val_acc: 0.7730\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8806 - acc: 0.6771 - val_loss: 0.6452 - val_acc: 0.7690\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8787 - acc: 0.6729 - val_loss: 0.6458 - val_acc: 0.7680\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8810 - acc: 0.6667 - val_loss: 0.6410 - val_acc: 0.7690\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8781 - acc: 0.6759 - val_loss: 0.6393 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8585 - acc: 0.6837 - val_loss: 0.6353 - val_acc: 0.7760\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8769 - acc: 0.6697 - val_loss: 0.6348 - val_acc: 0.7730\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8636 - acc: 0.6739 - val_loss: 0.6307 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8625 - acc: 0.6783 - val_loss: 0.6330 - val_acc: 0.7690\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8680 - acc: 0.6745 - val_loss: 0.6301 - val_acc: 0.7690\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8501 - acc: 0.6813 - val_loss: 0.6255 - val_acc: 0.7760\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8556 - acc: 0.6827 - val_loss: 0.6259 - val_acc: 0.7750\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8467 - acc: 0.6856 - val_loss: 0.6193 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8467 - acc: 0.6855 - val_loss: 0.6211 - val_acc: 0.7790\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8501 - acc: 0.6861 - val_loss: 0.6207 - val_acc: 0.7720\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8459 - acc: 0.6893 - val_loss: 0.6175 - val_acc: 0.7730\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8409 - acc: 0.6871 - val_loss: 0.6143 - val_acc: 0.7750\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8258 - acc: 0.6935 - val_loss: 0.6071 - val_acc: 0.7740\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8438 - acc: 0.6811 - val_loss: 0.6106 - val_acc: 0.7760\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8462 - acc: 0.6849 - val_loss: 0.6095 - val_acc: 0.7780\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8464 - acc: 0.6879 - val_loss: 0.6135 - val_acc: 0.7700\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8320 - acc: 0.6905 - val_loss: 0.6064 - val_acc: 0.7780\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8344 - acc: 0.6872 - val_loss: 0.6050 - val_acc: 0.7790\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8256 - acc: 0.6895 - val_loss: 0.6003 - val_acc: 0.7830\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8262 - acc: 0.6869 - val_loss: 0.6032 - val_acc: 0.7750\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8167 - acc: 0.6940 - val_loss: 0.6005 - val_acc: 0.7780\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8205 - acc: 0.6971 - val_loss: 0.6007 - val_acc: 0.7720\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8223 - acc: 0.6901 - val_loss: 0.5957 - val_acc: 0.7800\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8141 - acc: 0.6872 - val_loss: 0.5986 - val_acc: 0.7760\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8166 - acc: 0.6963 - val_loss: 0.6004 - val_acc: 0.7800\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8121 - acc: 0.6956 - val_loss: 0.5935 - val_acc: 0.7800\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8188 - acc: 0.6909 - val_loss: 0.5967 - val_acc: 0.7740\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8121 - acc: 0.6903 - val_loss: 0.5940 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8256 - acc: 0.6869 - val_loss: 0.5929 - val_acc: 0.7800\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8162 - acc: 0.6953 - val_loss: 0.5958 - val_acc: 0.7750\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8145 - acc: 0.6939 - val_loss: 0.5926 - val_acc: 0.7800\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7851 - acc: 0.7027 - val_loss: 0.5859 - val_acc: 0.7810\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8024 - acc: 0.6997 - val_loss: 0.5888 - val_acc: 0.7790\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8118 - acc: 0.6956 - val_loss: 0.5856 - val_acc: 0.7800\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7992 - acc: 0.6975 - val_loss: 0.5863 - val_acc: 0.7830\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7884 - acc: 0.7021 - val_loss: 0.5878 - val_acc: 0.7790\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7951 - acc: 0.6989 - val_loss: 0.5807 - val_acc: 0.7850\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7990 - acc: 0.7021 - val_loss: 0.5814 - val_acc: 0.7820\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7917 - acc: 0.7055 - val_loss: 0.5799 - val_acc: 0.7820\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7896 - acc: 0.6977 - val_loss: 0.5804 - val_acc: 0.7800\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7846 - acc: 0.7075 - val_loss: 0.5777 - val_acc: 0.7820\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7758 - acc: 0.7072 - val_loss: 0.5758 - val_acc: 0.7830\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7869 - acc: 0.7036 - val_loss: 0.5753 - val_acc: 0.7810\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7991 - acc: 0.7007 - val_loss: 0.5782 - val_acc: 0.7780\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7766 - acc: 0.7100 - val_loss: 0.5734 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7900 - acc: 0.7077 - val_loss: 0.5761 - val_acc: 0.7790\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7828 - acc: 0.7041 - val_loss: 0.5749 - val_acc: 0.7810\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7735 - acc: 0.7017 - val_loss: 0.5731 - val_acc: 0.7830\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7864 - acc: 0.6967 - val_loss: 0.5725 - val_acc: 0.7850\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7709 - acc: 0.7065 - val_loss: 0.5725 - val_acc: 0.7850\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7618 - acc: 0.7193 - val_loss: 0.5738 - val_acc: 0.7860\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7735 - acc: 0.7088 - val_loss: 0.5714 - val_acc: 0.7790\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7815 - acc: 0.7080 - val_loss: 0.5681 - val_acc: 0.7830\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7633 - acc: 0.7135 - val_loss: 0.5700 - val_acc: 0.7790\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7770 - acc: 0.7081 - val_loss: 0.5646 - val_acc: 0.7850\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7687 - acc: 0.7065 - val_loss: 0.5652 - val_acc: 0.7830\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7664 - acc: 0.7081 - val_loss: 0.5663 - val_acc: 0.7820\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7511 - acc: 0.7199 - val_loss: 0.5640 - val_acc: 0.7870\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7603 - acc: 0.7092 - val_loss: 0.5615 - val_acc: 0.7850\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7543 - acc: 0.7159 - val_loss: 0.5584 - val_acc: 0.7890\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7554 - acc: 0.7217 - val_loss: 0.5577 - val_acc: 0.7880\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7407 - acc: 0.7245 - val_loss: 0.5528 - val_acc: 0.7900\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 56us/step\n",
      "1500/1500 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.467536919093132, 0.8434666666666667]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6155827878316243, 0.7673333338101704]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.9289 - acc: 0.1855 - val_loss: 1.8808 - val_acc: 0.2537\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.8268 - acc: 0.3065 - val_loss: 1.7616 - val_acc: 0.3540\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.6706 - acc: 0.4198 - val_loss: 1.5799 - val_acc: 0.4700\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.4701 - acc: 0.5186 - val_loss: 1.3816 - val_acc: 0.5550\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.2729 - acc: 0.5987 - val_loss: 1.1987 - val_acc: 0.6177\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.1022 - acc: 0.6524 - val_loss: 1.0536 - val_acc: 0.6557\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.9713 - acc: 0.6855 - val_loss: 0.9484 - val_acc: 0.6793\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.8766 - acc: 0.7076 - val_loss: 0.8738 - val_acc: 0.6950\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.8088 - acc: 0.7207 - val_loss: 0.8210 - val_acc: 0.7047\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.7592 - acc: 0.7343 - val_loss: 0.7824 - val_acc: 0.7153\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.7212 - acc: 0.7423 - val_loss: 0.7558 - val_acc: 0.7230\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6917 - acc: 0.7505 - val_loss: 0.7315 - val_acc: 0.7267\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6678 - acc: 0.7579 - val_loss: 0.7167 - val_acc: 0.7287\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.7637 - val_loss: 0.6985 - val_acc: 0.7423\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6315 - acc: 0.7691 - val_loss: 0.6888 - val_acc: 0.7403\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.6167 - acc: 0.7751 - val_loss: 0.6753 - val_acc: 0.7493\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.6042 - acc: 0.7783 - val_loss: 0.6658 - val_acc: 0.7507\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5925 - acc: 0.7833 - val_loss: 0.6572 - val_acc: 0.7587\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5821 - acc: 0.7872 - val_loss: 0.6512 - val_acc: 0.7603\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5726 - acc: 0.7903 - val_loss: 0.6442 - val_acc: 0.7627\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5639 - acc: 0.7939 - val_loss: 0.6381 - val_acc: 0.7620\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5555 - acc: 0.7981 - val_loss: 0.6341 - val_acc: 0.7613\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5481 - acc: 0.7999 - val_loss: 0.6293 - val_acc: 0.7700\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5409 - acc: 0.8030 - val_loss: 0.6230 - val_acc: 0.7710\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.5339 - acc: 0.8056 - val_loss: 0.6179 - val_acc: 0.7733\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5279 - acc: 0.8078 - val_loss: 0.6157 - val_acc: 0.7717\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5214 - acc: 0.8101 - val_loss: 0.6118 - val_acc: 0.7780\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5156 - acc: 0.8134 - val_loss: 0.6067 - val_acc: 0.7763\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5104 - acc: 0.8147 - val_loss: 0.6039 - val_acc: 0.7833\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5051 - acc: 0.8167 - val_loss: 0.6003 - val_acc: 0.7833\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4999 - acc: 0.8178 - val_loss: 0.5969 - val_acc: 0.7830\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4946 - acc: 0.8205 - val_loss: 0.5947 - val_acc: 0.7810\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4900 - acc: 0.8213 - val_loss: 0.5924 - val_acc: 0.7893\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4855 - acc: 0.8238 - val_loss: 0.5928 - val_acc: 0.7837\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4811 - acc: 0.8268 - val_loss: 0.5886 - val_acc: 0.7817\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4767 - acc: 0.8283 - val_loss: 0.5862 - val_acc: 0.7910\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4726 - acc: 0.8288 - val_loss: 0.5839 - val_acc: 0.7873\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4686 - acc: 0.8308 - val_loss: 0.5833 - val_acc: 0.7940\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4645 - acc: 0.8322 - val_loss: 0.5809 - val_acc: 0.7913\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4611 - acc: 0.8347 - val_loss: 0.5767 - val_acc: 0.7927\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4573 - acc: 0.8352 - val_loss: 0.5777 - val_acc: 0.7957\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4538 - acc: 0.8372 - val_loss: 0.5787 - val_acc: 0.7900\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4501 - acc: 0.8390 - val_loss: 0.5749 - val_acc: 0.7910\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4466 - acc: 0.8397 - val_loss: 0.5746 - val_acc: 0.7963\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4435 - acc: 0.8417 - val_loss: 0.5697 - val_acc: 0.7950\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4400 - acc: 0.8417 - val_loss: 0.5710 - val_acc: 0.7967\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4369 - acc: 0.8434 - val_loss: 0.5703 - val_acc: 0.7990\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4340 - acc: 0.8453 - val_loss: 0.5696 - val_acc: 0.7950\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4308 - acc: 0.8458 - val_loss: 0.5703 - val_acc: 0.7947\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4275 - acc: 0.8475 - val_loss: 0.5662 - val_acc: 0.7997\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4248 - acc: 0.8495 - val_loss: 0.5660 - val_acc: 0.7997\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4219 - acc: 0.8491 - val_loss: 0.5722 - val_acc: 0.7930\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4188 - acc: 0.8511 - val_loss: 0.5685 - val_acc: 0.7983\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4165 - acc: 0.8521 - val_loss: 0.5652 - val_acc: 0.7997\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4136 - acc: 0.8527 - val_loss: 0.5649 - val_acc: 0.7990\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4112 - acc: 0.8543 - val_loss: 0.5635 - val_acc: 0.8017\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4086 - acc: 0.8552 - val_loss: 0.5648 - val_acc: 0.7967\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4061 - acc: 0.8568 - val_loss: 0.5658 - val_acc: 0.7977\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4040 - acc: 0.8569 - val_loss: 0.5635 - val_acc: 0.7993\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4015 - acc: 0.8572 - val_loss: 0.5638 - val_acc: 0.8013\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3987 - acc: 0.8599 - val_loss: 0.5646 - val_acc: 0.7980\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3966 - acc: 0.8619 - val_loss: 0.5612 - val_acc: 0.8037\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3941 - acc: 0.8619 - val_loss: 0.5630 - val_acc: 0.8000\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3924 - acc: 0.8615 - val_loss: 0.5667 - val_acc: 0.7983\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3898 - acc: 0.8635 - val_loss: 0.5606 - val_acc: 0.8013\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3878 - acc: 0.8638 - val_loss: 0.5641 - val_acc: 0.7997\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3853 - acc: 0.8648 - val_loss: 0.5627 - val_acc: 0.8000\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3832 - acc: 0.8648 - val_loss: 0.5623 - val_acc: 0.8010\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3811 - acc: 0.8668 - val_loss: 0.5616 - val_acc: 0.8023\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3791 - acc: 0.8667 - val_loss: 0.5643 - val_acc: 0.8000\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3772 - acc: 0.8677 - val_loss: 0.5623 - val_acc: 0.7980\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3751 - acc: 0.8685 - val_loss: 0.5656 - val_acc: 0.8010\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3733 - acc: 0.8683 - val_loss: 0.5644 - val_acc: 0.7990\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3712 - acc: 0.8695 - val_loss: 0.5652 - val_acc: 0.7990\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3696 - acc: 0.8700 - val_loss: 0.5660 - val_acc: 0.8010\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3673 - acc: 0.8711 - val_loss: 0.5645 - val_acc: 0.8027\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3656 - acc: 0.8716 - val_loss: 0.5632 - val_acc: 0.8023\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3638 - acc: 0.8718 - val_loss: 0.5643 - val_acc: 0.7990\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3619 - acc: 0.8715 - val_loss: 0.5641 - val_acc: 0.8023\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3602 - acc: 0.8728 - val_loss: 0.5653 - val_acc: 0.8027\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3581 - acc: 0.8737 - val_loss: 0.5700 - val_acc: 0.7973\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3566 - acc: 0.8748 - val_loss: 0.5688 - val_acc: 0.7997\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3552 - acc: 0.8756 - val_loss: 0.5680 - val_acc: 0.8013\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3534 - acc: 0.8758 - val_loss: 0.5711 - val_acc: 0.7940\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3516 - acc: 0.8768 - val_loss: 0.5706 - val_acc: 0.7997\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3499 - acc: 0.8772 - val_loss: 0.5685 - val_acc: 0.7993\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3480 - acc: 0.8779 - val_loss: 0.5693 - val_acc: 0.8003\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3468 - acc: 0.8791 - val_loss: 0.5682 - val_acc: 0.8020\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3448 - acc: 0.8793 - val_loss: 0.5685 - val_acc: 0.8010\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3436 - acc: 0.8799 - val_loss: 0.5680 - val_acc: 0.7997\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3420 - acc: 0.8795 - val_loss: 0.5712 - val_acc: 0.8020\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3403 - acc: 0.8810 - val_loss: 0.5697 - val_acc: 0.8007\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3387 - acc: 0.8804 - val_loss: 0.5781 - val_acc: 0.7957\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3370 - acc: 0.8820 - val_loss: 0.5761 - val_acc: 0.7953\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3354 - acc: 0.8821 - val_loss: 0.5749 - val_acc: 0.7997\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3342 - acc: 0.8828 - val_loss: 0.5741 - val_acc: 0.8003\n",
      "Epoch 97/120\n",
      " 9216/33000 [=======>......................] - ETA: 0s - loss: 0.3218 - acc: 0.8880"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
